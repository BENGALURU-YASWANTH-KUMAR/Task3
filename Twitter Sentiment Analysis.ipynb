{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0d5cb4f-6cac-4d51-8b29-f2cae7128d89",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŒŸ Decoding Digital Emotions: A Journey into Social Media Sentiment Analysis\n",
    "\n",
    "## ðŸ“Š Project Overview\n",
    "\n",
    "Welcome to the cutting edge of digital emotion decryption! Our mission is to dive deep into the ocean of social media data, extracting, analyzing, and interpreting user sentiments. We're building a powerful sentiment analysis model that classifies social media posts as positive, negative, or neutral, transforming raw text into actionable insights for brand reputation management and market research.\n",
    "\n",
    "## ðŸ§  The Brain Behind the Magic\n",
    "\n",
    "### 1. Data Acquisition and Preprocessing\n",
    "\n",
    "We're tapping into the Sentiment140 dataset, a treasure trove of 1.6 million tweets, perfect for training our sentiment detection AI.\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Kaggle dataset integration\n",
    "!pip install kaggle\n",
    "os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "!cp kaggle.json ~/.kaggle/kaggle.json\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "!kaggle datasets download -d kazanova/sentiment140\n",
    "\n",
    "# Loading our digital gold\n",
    "column_names=['target','ids','date','flag','user','text']\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', \n",
    "                 names=column_names, encoding='latin-1')\n",
    "```\n",
    "\n",
    "### 2. Data Exploration and Cleaning\n",
    "\n",
    "Before unleashing our AI, we need to understand our data. We're checking for missing values and exploring the distribution of sentiments.\n",
    "\n",
    "```python\n",
    "df.isnull().sum()\n",
    "df['target'].value_counts()\n",
    "```\n",
    "\n",
    "### 3. Text Preprocessing\n",
    "\n",
    "Raw text is like a rough diamond - we need to polish it. We're using NLTK for removing stopwords and stemming, crucial steps in preparing our text for analysis.\n",
    "\n",
    "```python\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "nltk.download('stopwords')\n",
    "```\n",
    "\n",
    "### 4. Feature Extraction\n",
    "\n",
    "We're using TF-IDF (Term Frequency-Inverse Document Frequency) to convert our text into a format our model can understand. This technique helps us capture the importance of words in the context of our entire dataset.\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "```\n",
    "\n",
    "### 5. Model Building and Evaluation\n",
    "\n",
    "We're employing Logistic Regression for its simplicity and effectiveness in text classification tasks. Our model will be trained on a portion of the data and tested on the rest to ensure its real-world performance.\n",
    "\n",
    "```python\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score\n",
    "```\n",
    "\n",
    "## ðŸŽ¯ Why This Matters\n",
    "\n",
    "In the age of social media, understanding public sentiment is like having a superpower. Our project isn't just about classification - it's about giving businesses the ability to:\n",
    "\n",
    "- ðŸ“ˆ Monitor brand health in real-time\n",
    "- ðŸŽ¯ Tailor marketing strategies based on public mood\n",
    "- ðŸ›  Improve products and services through direct feedback\n",
    "- ðŸš¨ Detect and address PR crises before they escalate\n",
    "\n",
    "## ðŸ”® Looking Ahead\n",
    "\n",
    "As we refine our model, we're not just improving accuracy - we're paving the way for more nuanced sentiment analysis. Future iterations could include:\n",
    "\n",
    "- ðŸ˜ŠðŸ˜ðŸ˜¢ Multi-class sentiment classification\n",
    "- ðŸ“Š Sentiment trend analysis over time\n",
    "- ðŸ—º Geospatial sentiment mapping\n",
    "\n",
    "Join us on this exciting journey as we turn the cacophony of social media into a symphony of insights!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b53399aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "10d1ea46",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3be2b2d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e740422a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: kaggle in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.6.17)\n",
      "Requirement already satisfied: six>=1.10 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (1.16.0)\n",
      "Requirement already satisfied: certifi>=2023.7.22 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2024.2.2)\n",
      "Requirement already satisfied: python-dateutil in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.9.0.post0)\n",
      "Requirement already satisfied: requests in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.31.0)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (4.66.5)\n",
      "Requirement already satisfied: python-slugify in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (8.0.4)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (2.2.1)\n",
      "Requirement already satisfied: bleach in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from kaggle) (6.1.0)\n",
      "Requirement already satisfied: webencodings in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from bleach->kaggle) (0.5.1)\n",
      "Requirement already satisfied: text-unidecode>=1.3 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-slugify->kaggle) (1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kaggle) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from requests->kaggle) (3.6)\n",
      "Requirement already satisfied: colorama in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from tqdm->kaggle) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install kaggle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c8f34fba",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "cp: cannot stat 'kaggle.json': No such file or directory\n",
      "chmod: cannot access '~/.kaggle/kaggle.json': No such file or directory\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/kazanova/sentiment140\n",
      "License(s): other\n",
      "sentiment140.zip: Skipping, found more recently modified local copy (use --force to force download)\n",
      "   0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY _TheSpecialOne_  \\\n",
      "0  0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   scotthamilton   \n",
      "1  0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY        mattycus   \n",
      "2  0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY         ElleCTF   \n",
      "3  0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY          Karoli   \n",
      "4  0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY        joy_wolf   \n",
      "\n",
      "  @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
      "0  is upset that he can't update his Facebook by ...                                                                   \n",
      "1  @Kenichan I dived many times for the ball. Man...                                                                   \n",
      "2    my whole body feels itchy and like its on fire                                                                    \n",
      "3  @nationwideclass no, it's not behaving at all....                                                                   \n",
      "4                      @Kwesidei not the whole crew                                                                    \n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "os.makedirs(os.path.expanduser('~/.kaggle'), exist_ok=True)\n",
    "\n",
    "!cp kaggle.json ~/.kaggle/kaggle.json\n",
    "\n",
    "!chmod 600 ~/.kaggle/kaggle.json\n",
    "\n",
    "!kaggle datasets download -d kazanova/sentiment140\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv', encoding='latin-1')\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4b478ed7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1467810369</th>\n",
       "      <th>Mon Apr 06 22:19:45 PDT 2009</th>\n",
       "      <th>NO_QUERY</th>\n",
       "      <th>_TheSpecialOne_</th>\n",
       "      <th>@switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811372</td>\n",
       "      <td>Mon Apr 06 22:20:00 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>joy_wolf</td>\n",
       "      <td>@Kwesidei not the whole crew</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599994</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599999 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY  \\\n",
       "0        0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "1        0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "2        0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "3        0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4        0  1467811372  Mon Apr 06 22:20:00 PDT 2009  NO_QUERY   \n",
       "...     ..         ...                           ...       ...   \n",
       "1599994  4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599995  4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996  4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997  4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998  4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "         _TheSpecialOne_  \\\n",
       "0          scotthamilton   \n",
       "1               mattycus   \n",
       "2                ElleCTF   \n",
       "3                 Karoli   \n",
       "4               joy_wolf   \n",
       "...                  ...   \n",
       "1599994  AmandaMarie1028   \n",
       "1599995      TheWDBoards   \n",
       "1599996           bpbabe   \n",
       "1599997     tinydiamondz   \n",
       "1599998   RyanTrevMorris   \n",
       "\n",
       "        @switchfoot http://twitpic.com/2y1zl - Awww, that's a bummer.  You shoulda got David Carr of Third Day to do it. ;D  \n",
       "0        is upset that he can't update his Facebook by ...                                                                   \n",
       "1        @Kenichan I dived many times for the ball. Man...                                                                   \n",
       "2          my whole body feels itchy and like its on fire                                                                    \n",
       "3        @nationwideclass no, it's not behaving at all....                                                                   \n",
       "4                            @Kwesidei not the whole crew                                                                    \n",
       "...                                                    ...                                                                   \n",
       "1599994  Just woke up. Having no school is the best fee...                                                                   \n",
       "1599995  TheWDB.com - Very cool to hear old Walt interv...                                                                   \n",
       "1599996  Are you ready for your MoJo Makeover? Ask me f...                                                                   \n",
       "1599997  Happy 38th Birthday to my boo of alll time!!! ...                                                                   \n",
       "1599998  happy #charitytuesday @theNSPCC @SparksCharity...                                                                   \n",
       "\n",
       "[1599999 rows x 6 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "c35cada8",
   "metadata": {},
   "outputs": [],
   "source": [
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3380f154-92d9-460e-8228-04647f63a2a0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: nltk in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (3.8.1)\n",
      "Requirement already satisfied: click in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (1.3.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (2024.7.24)\n",
      "Requirement already satisfied: tqdm in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from nltk) (4.66.5)\n",
      "Requirement already satisfied: colorama in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from click->nltk) (0.4.6)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "c8336ee2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "86733513",
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c233afc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import TfidfVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "486f77a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ea46a0b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "9fe6d4dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4fb51cd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['i', 'me', 'my', 'myself', 'we', 'our', 'ours', 'ourselves', 'you', \"you're\", \"you've\", \"you'll\", \"you'd\", 'your', 'yours', 'yourself', 'yourselves', 'he', 'him', 'his', 'himself', 'she', \"she's\", 'her', 'hers', 'herself', 'it', \"it's\", 'its', 'itself', 'they', 'them', 'their', 'theirs', 'themselves', 'what', 'which', 'who', 'whom', 'this', 'that', \"that'll\", 'these', 'those', 'am', 'is', 'are', 'was', 'were', 'be', 'been', 'being', 'have', 'has', 'had', 'having', 'do', 'does', 'did', 'doing', 'a', 'an', 'the', 'and', 'but', 'if', 'or', 'because', 'as', 'until', 'while', 'of', 'at', 'by', 'for', 'with', 'about', 'against', 'between', 'into', 'through', 'during', 'before', 'after', 'above', 'below', 'to', 'from', 'up', 'down', 'in', 'out', 'on', 'off', 'over', 'under', 'again', 'further', 'then', 'once', 'here', 'there', 'when', 'where', 'why', 'how', 'all', 'any', 'both', 'each', 'few', 'more', 'most', 'other', 'some', 'such', 'no', 'nor', 'not', 'only', 'own', 'same', 'so', 'than', 'too', 'very', 's', 't', 'can', 'will', 'just', 'don', \"don't\", 'should', \"should've\", 'now', 'd', 'll', 'm', 'o', 're', 've', 'y', 'ain', 'aren', \"aren't\", 'couldn', \"couldn't\", 'didn', \"didn't\", 'doesn', \"doesn't\", 'hadn', \"hadn't\", 'hasn', \"hasn't\", 'haven', \"haven't\", 'isn', \"isn't\", 'ma', 'mightn', \"mightn't\", 'mustn', \"mustn't\", 'needn', \"needn't\", 'shan', \"shan't\", 'shouldn', \"shouldn't\", 'wasn', \"wasn't\", 'weren', \"weren't\", 'won', \"won't\", 'wouldn', \"wouldn't\"]\n"
     ]
    }
   ],
   "source": [
    "print(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "68be3b99",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_names=['target','ids','date','flag','user','text']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "46cc5f55",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "column_names=['target','ids','date','flag','user','text']\n",
    "# Adjust the filename if necessary\n",
    "df = pd.read_csv('training.1600000.processed.noemoticon.csv',names=column_names, encoding='latin-1')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9e055d5a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...  \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...  \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...  \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire   \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....  \n",
       "...                  ...                                                ...  \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...  \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...  \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...  \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...  \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...  \n",
       "\n",
       "[1600000 rows x 6 columns]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b0ab5beb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target    0\n",
       "ids       0\n",
       "date      0\n",
       "flag      0\n",
       "user      0\n",
       "text      0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "b0ec1a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "13726200",
   "metadata": {},
   "outputs": [],
   "source": [
    "a=10"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f32a8e7-9b90-4b1e-b7d9-9d3b1bdb354f",
   "metadata": {},
   "source": [
    "\n",
    "# ðŸŽ­ Sentiment Scoring and Text Preprocessing: Unveiling the Emotions in Tweets\n",
    "\n",
    "## ðŸ’¡ Sentiment Analysis: Beyond Binary Classification\n",
    "\n",
    "We've taken our sentiment analysis to the next level by introducing a more nuanced approach:\n",
    "\n",
    "```python\n",
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "df['sentiment_score'] = df['text'].apply(get_sentiment_score)\n",
    "```\n",
    "\n",
    "### ðŸŽ¨ Introducing Neutral Sentiment\n",
    "\n",
    "We've expanded our classification to include a neutral category:\n",
    "\n",
    "```python\n",
    "NEUTRAL_THRESHOLD = 0.05\n",
    "df['new_target'] = df['target'].copy()\n",
    "df.loc[(df['sentiment_score'] >= -NEUTRAL_THRESHOLD) & (df['sentiment_score'] <= NEUTRAL_THRESHOLD), 'new_target'] = 2\n",
    "df['new_target'] = df['new_target'].map({0: 0, 2: 2, 4: 4})\n",
    "```\n",
    "\n",
    "### ðŸ“Š Distribution of Sentiments\n",
    "\n",
    "Our refined approach yields a more balanced distribution of sentiments:\n",
    "\n",
    "```\n",
    "new_target\n",
    "0    584568 (Negative)\n",
    "4    571670 (Positive)\n",
    "2    443762 (Neutral)\n",
    "```\n",
    "\n",
    "## ðŸ§¹ Text Preprocessing: Cleaning and Standardizing\n",
    "\n",
    "### ðŸŒ± Stemming: Back to the Roots\n",
    "\n",
    "We're using the Porter Stemmer to reduce words to their root form:\n",
    "\n",
    "```python\n",
    "from nltk.stem import PorterStemmer\n",
    "port_stem = PorterStemmer()\n",
    "\n",
    "def stemming(content):\n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower().split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if word not in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content\n",
    "```\n",
    "\n",
    "### âš¡ Performance Optimization\n",
    "\n",
    "We've optimized our preprocessing to handle the entire dataset efficiently:\n",
    "\n",
    "```python\n",
    "start_time = time.time()\n",
    "df['stemmed_content'] = df['text'].apply(stemming)\n",
    "end_time = time.time()\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Time taken to process 1,600,000 rows: {execution_time:.2f} seconds\")\n",
    "```\n",
    "\n",
    "## ðŸ“Š Data Insights\n",
    "\n",
    "After preprocessing, we gain valuable insights into our dataset:\n",
    "\n",
    "- Shape of the dataframe: 1,600,000 rows x 8 columns\n",
    "- Memory usage: Efficiently managed large dataset\n",
    "- Null values: Ensured data integrity with no null values in stemmed_content\n",
    "\n",
    "## ðŸš€ Why This Matters\n",
    "\n",
    "1. **Nuanced Sentiment Analysis**: By introducing a neutral category, we capture the full spectrum of emotions in social media.\n",
    "2. **Efficient Text Processing**: Our optimized stemming process handles millions of tweets quickly, making real-time analysis possible.\n",
    "3. **Data Quality**: Rigorous preprocessing ensures our model works with clean, standardized text data.\n",
    "\n",
    "## ðŸ”® Looking Ahead\n",
    "\n",
    "- Explore advanced NLP techniques like lemmatization or word embeddings\n",
    "- Implement multi-language support for global sentiment analysis\n",
    "- Develop real-time streaming capabilities for instant sentiment tracking\n",
    "\n",
    "This refined approach to sentiment analysis and text preprocessing sets the stage for highly accurate and insightful social media sentiment analysis, providing businesses with a powerful tool for understanding public opinion in the digital age."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c71a7024",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package vader_lexicon to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package vader_lexicon is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "new_target\n",
      "0    584568\n",
      "4    571670\n",
      "2    443762\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "from nltk.sentiment import SentimentIntensityAnalyzer\n",
    "\n",
    "nltk.download('vader_lexicon')\n",
    "\n",
    "sia = SentimentIntensityAnalyzer()\n",
    "\n",
    "def get_sentiment_score(text):\n",
    "    return sia.polarity_scores(text)['compound']\n",
    "\n",
    "df['sentiment_score'] = df['text'].apply(get_sentiment_score)\n",
    "\n",
    "NEUTRAL_THRESHOLD = 0.05\n",
    "\n",
    "df['new_target'] = df['target'].copy()\n",
    "df.loc[(df['sentiment_score'] >= -NEUTRAL_THRESHOLD) & (df['sentiment_score'] <= NEUTRAL_THRESHOLD), 'new_target'] = 2\n",
    "\n",
    "df['new_target'] = df['new_target'].map({0: 0, 2: 2, 4: 4})\n",
    "\n",
    "print(df['new_target'].value_counts())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "b0629131",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    800000\n",
       "4    800000\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "78c3c80f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>new_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>-0.0173</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>-0.7500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>0.4376</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>0.6784</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \\\n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "...                  ...                                                ...   \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...   \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "         sentiment_score  new_target  \n",
       "0                -0.0173           2  \n",
       "1                -0.7500           0  \n",
       "2                 0.4939           0  \n",
       "3                -0.2500           0  \n",
       "4                -0.6597           0  \n",
       "...                  ...         ...  \n",
       "1599995           0.5423           4  \n",
       "1599996           0.4376           4  \n",
       "1599997           0.3612           4  \n",
       "1599998           0.6784           4  \n",
       "1599999           0.5719           4  \n",
       "\n",
       "[1600000 rows x 8 columns]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "819e1a68",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target'] = df['new_target']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "7623613f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>new_target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>-0.0173</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>-0.7500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>0.4376</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>4</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>0.6784</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>4</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "0             2  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1             0  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2             0  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3             0  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4             0  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       4  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       4  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       4  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       4  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       4  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \\\n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "...                  ...                                                ...   \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...   \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "         sentiment_score  new_target  \n",
       "0                -0.0173           2  \n",
       "1                -0.7500           0  \n",
       "2                 0.4939           0  \n",
       "3                -0.2500           0  \n",
       "4                -0.6597           0  \n",
       "...                  ...         ...  \n",
       "1599995           0.5423           4  \n",
       "1599996           0.4376           4  \n",
       "1599997           0.3612           4  \n",
       "1599998           0.6784           4  \n",
       "1599999           0.5719           4  \n",
       "\n",
       "[1600000 rows x 8 columns]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "23cc520e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    584568\n",
       "4    571670\n",
       "2    443762\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "4252bb0d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Index(['target', 'ids', 'date', 'flag', 'user', 'text', 'sentiment_score'], dtype='object')\n",
      "target\n",
      "0    584568\n",
      "4    571670\n",
      "2    443762\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "df = df.drop('new_target', axis=1)\n",
    "\n",
    "print(df.columns)\n",
    "print(df['target'].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4638c81",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "target\n",
       "0    584568\n",
       "4    571670\n",
       "2    443762\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['target'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "930adeb8",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['target']=df['target'].replace({\n",
    "    0:-1,\n",
    "    4:1,\n",
    "    2:0\n",
    "})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "a9e26da3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>-0.0173</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>-0.7500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.4939</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>-0.2500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>-0.6597</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>0.5423</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>0.4376</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>0.3612</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>0.6784</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>0.5719</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1            -1  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2            -1  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3            -1  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4            -1  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       1  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       1  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       1  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       1  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       1  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \\\n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "...                  ...                                                ...   \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...   \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "         sentiment_score  \n",
       "0                -0.0173  \n",
       "1                -0.7500  \n",
       "2                 0.4939  \n",
       "3                -0.2500  \n",
       "4                -0.6597  \n",
       "...                  ...  \n",
       "1599995           0.5423  \n",
       "1599996           0.4376  \n",
       "1599997           0.3612  \n",
       "1599998           0.6784  \n",
       "1599999           0.5719  \n",
       "\n",
       "[1600000 rows x 7 columns]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d06b4f",
   "metadata": {},
   "source": [
    "-1---> Negative Tweet\n",
    "\n",
    "0----> Neutral Tweet\n",
    "\n",
    "1---> Positive Tweet\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "830ac5e9",
   "metadata": {},
   "source": [
    "# stemming"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f138b27",
   "metadata": {},
   "source": [
    "stemming is a process of redung a word to its Root word"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5392fca7",
   "metadata": {},
   "source": [
    "example:actor,acting,actress=act"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "de5f2b5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "port_stem=PorterStemmer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "03d3cb98",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=20\n",
    "a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "1e3bdec2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process 100 rows: 0.5224 seconds\n",
      "                                                text  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1  is upset that he can't update his Facebook by ...   \n",
      "2  @Kenichan I dived many times for the ball. Man...   \n",
      "3    my whole body feels itchy and like its on fire    \n",
      "4  @nationwideclass no, it's not behaving at all....   \n",
      "\n",
      "                                     stemmed_content  \n",
      "0  switchfoot http twitpic com zl awww bummer sho...  \n",
      "1  upset updat facebook text might cri result sch...  \n",
      "2  kenichan dive mani time ball manag save rest g...  \n",
      "3                    whole bodi feel itchi like fire  \n",
      "4                      nationwideclass behav mad see  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yaswa\\AppData\\Local\\Temp\\ipykernel_1516\\1580249403.py:22: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  df_sample['stemmed_content'] = df_sample['text'].apply(stemming)\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import time\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "port_stem = PorterStemmer()\n",
    "\n",
    "def stemming(content):\n",
    "    \n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "    stemmed_content = stemmed_content.lower().split()\n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if word not in stopwords.words('english')]\n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content\n",
    "\n",
    "df_sample = df.head(100)\n",
    "\n",
    "start_time = time.time()\n",
    "df_sample['stemmed_content'] = df_sample['text'].apply(stemming)\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "print(f\"Time taken to process 100 rows: {execution_time:.4f} seconds\")\n",
    "\n",
    "print(df_sample[['text', 'stemmed_content']].head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9a8053f1-8f64-433a-885d-4de639a1f0cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1600000 entries, 0 to 1599999\n",
      "Data columns (total 7 columns):\n",
      " #   Column           Non-Null Count    Dtype  \n",
      "---  ------           --------------    -----  \n",
      " 0   target           1600000 non-null  int64  \n",
      " 1   ids              1600000 non-null  int64  \n",
      " 2   date             1600000 non-null  object \n",
      " 3   flag             1600000 non-null  object \n",
      " 4   user             1600000 non-null  object \n",
      " 5   text             1600000 non-null  object \n",
      " 6   sentiment_score  1600000 non-null  float64\n",
      "dtypes: float64(1), int64(2), object(4)\n",
      "memory usage: 85.4+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "f256a7d0-0196-495d-9520-cbc3e5a9af2b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>sentiment_score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>1.600000e+06</td>\n",
       "      <td>1.600000e+06</td>\n",
       "      <td>1.600000e+06</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>-8.061250e-03</td>\n",
       "      <td>1.998818e+09</td>\n",
       "      <td>1.411054e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>8.500495e-01</td>\n",
       "      <td>1.935761e+08</td>\n",
       "      <td>4.572251e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.467810e+09</td>\n",
       "      <td>-9.985000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>-1.000000e+00</td>\n",
       "      <td>1.956916e+09</td>\n",
       "      <td>-7.720000e-02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000e+00</td>\n",
       "      <td>2.002102e+09</td>\n",
       "      <td>0.000000e+00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.177059e+09</td>\n",
       "      <td>5.267000e-01</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>2.329206e+09</td>\n",
       "      <td>9.987000e-01</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             target           ids  sentiment_score\n",
       "count  1.600000e+06  1.600000e+06     1.600000e+06\n",
       "mean  -8.061250e-03  1.998818e+09     1.411054e-01\n",
       "std    8.500495e-01  1.935761e+08     4.572251e-01\n",
       "min   -1.000000e+00  1.467810e+09    -9.985000e-01\n",
       "25%   -1.000000e+00  1.956916e+09    -7.720000e-02\n",
       "50%    0.000000e+00  2.002102e+09     0.000000e+00\n",
       "75%    1.000000e+00  2.177059e+09     5.267000e-01\n",
       "max    1.000000e+00  2.329206e+09     9.987000e-01"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "e39152f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\yaswa\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time taken to process 1,600,000 rows: 6114.01 seconds\n",
      "\n",
      "Sample of processed data:\n",
      "                                                text  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1  is upset that he can't update his Facebook by ...   \n",
      "2  @Kenichan I dived many times for the ball. Man...   \n",
      "3    my whole body feels itchy and like its on fire    \n",
      "4  @nationwideclass no, it's not behaving at all....   \n",
      "\n",
      "                                     stemmed_content  \n",
      "0  switchfoot http twitpic com zl awww bummer sho...  \n",
      "1  upset updat facebook text might cri result sch...  \n",
      "2  kenichan dive mani time ball manag save rest g...  \n",
      "3                    whole bodi feel itchi like fire  \n",
      "4                      nationwideclass behav mad see  \n",
      "\n",
      "Shape of the dataframe: (1600000, 8)\n",
      "\n",
      "Column names:\n",
      "['target', 'ids', 'date', 'flag', 'user', 'text', 'sentiment_score', 'stemmed_content']\n",
      "\n",
      "Number of null values in stemmed_content: 0\n",
      "\n",
      "Memory usage of the dataframe: 661.18 MB\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "import time\n",
    "import pandas as pd\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "nltk.download('stopwords')\n",
    "\n",
    "port_stem = PorterStemmer()\n",
    "\n",
    "def stemming(content):\n",
    "    \n",
    "    stemmed_content = re.sub('[^a-zA-Z]', ' ', content)\n",
    "   \n",
    "    stemmed_content = stemmed_content.lower().split()\n",
    "   \n",
    "    stemmed_content = [port_stem.stem(word) for word in stemmed_content if word not in stopwords.words('english')]\n",
    "    \n",
    "    stemmed_content = ' '.join(stemmed_content)\n",
    "    return stemmed_content\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "df['stemmed_content'] = df['text'].apply(stemming)\n",
    "\n",
    "end_time = time.time()\n",
    "\n",
    "execution_time = end_time - start_time\n",
    "\n",
    "print(f\"Time taken to process 1,600,000 rows: {execution_time:.2f} seconds\")\n",
    "\n",
    "df.to_csv('stemmed_tweets.csv', index=False)\n",
    "\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(df[['text', 'stemmed_content']].head())\n",
    "\n",
    "print(f\"\\nShape of the dataframe: {df.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "null_count = df['stemmed_content'].isnull().sum()\n",
    "print(f\"\\nNumber of null values in stemmed_content: {null_count}\")\n",
    "\n",
    "memory_usage = df.memory_usage(deep=True).sum() / 1024**2  # in MB\n",
    "print(f\"\\nMemory usage of the dataframe: {memory_usage:.2f} MB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46dd4dae-57de-4d18-8bd1-9e040b7e7997",
   "metadata": {},
   "source": [
    "# ðŸ§  Data Preparation and Feature Extraction: Transforming Text into Machine-Readable Insights\n",
    "\n",
    "## ðŸ“Š Loading and Verifying Processed Data\n",
    "\n",
    "We begin by loading our previously processed data, ensuring data integrity and consistency:\n",
    "\n",
    "```python\n",
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def load_processed_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading processed data from {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Processed data file not found: {file_path}\")\n",
    "\n",
    "processed_file = 'stemmed_tweets.csv'\n",
    "df = load_processed_data(processed_file)\n",
    "```\n",
    "\n",
    "### ðŸ” Data Quality Check\n",
    "\n",
    "We perform rigorous checks to ensure our data is ready for analysis:\n",
    "\n",
    "```python\n",
    "print(f\"\\nShape of the dataframe: {df.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "null_count = df['stemmed_content'].isnull().sum()\n",
    "print(f\"\\nNumber of null values in stemmed_content: {null_count}\")\n",
    "```\n",
    "\n",
    "### ðŸ§¹ Handling Missing Data\n",
    "\n",
    "We address any null values to maintain data integrity:\n",
    "\n",
    "```python\n",
    "df['stemmed_content'] = df['stemmed_content'].fillna('')\n",
    "null_count_after = df['stemmed_content'].isnull().sum()\n",
    "print(f\"\\nNumber of null values in stemmed_content after handling: {null_count_after}\")\n",
    "```\n",
    "\n",
    "## ðŸš€ Feature Extraction: TF-IDF Vectorization\n",
    "\n",
    "We transform our text data into a format suitable for machine learning:\n",
    "\n",
    "```python\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X = df['stemmed_content']\n",
    "y = df['target']\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1111, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "\n",
    "print(\"\\nShape of vectorized training data:\", X_train_vectorized.shape)\n",
    "print(\"Shape of vectorized testing data:\", X_test_vectorized.shape)\n",
    "```\n",
    "\n",
    "## ðŸŽ¯ Model Training and Evaluation\n",
    "\n",
    "We employ Logistic Regression for its efficiency in text classification:\n",
    "\n",
    "```python\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "log = LogisticRegression()\n",
    "log.fit(X_train_vectorized, y_train)\n",
    "\n",
    "accuracy = log.score(X_test_vectorized, y_test)\n",
    "print(f\"\\nModel Accuracy: {accuracy:.4f}\")\n",
    "```\n",
    "\n",
    "## ðŸ’¡ Key Insights\n",
    "\n",
    "1. **Robust Data Handling**: Our pipeline efficiently manages a large dataset of 1.6 million tweets.\n",
    "2. **Effective Feature Extraction**: TF-IDF vectorization captures the essence of our text data in 5000 features.\n",
    "3. **Solid Initial Performance**: Our Logistic Regression model achieves an accuracy of 0.7848, providing a strong baseline for sentiment classification.\n",
    "\n",
    "## ðŸš€ Why This Matters\n",
    "\n",
    "1. **Scalability**: Our approach can handle millions of tweets, essential for real-world social media analysis.\n",
    "2. **Interpretability**: TF-IDF features allow us to understand which words are most influential in sentiment classification.\n",
    "3. **Quick Iteration**: The efficiency of our pipeline allows for rapid experimentation and improvement.\n",
    "\n",
    "## ðŸ”® Looking Ahead\n",
    "\n",
    "- Experiment with advanced feature extraction techniques like word embeddings (Word2Vec, GloVe)\n",
    "- Explore ensemble methods or deep learning models to potentially improve accuracy\n",
    "- Implement cross-validation for more robust model evaluation\n",
    "- Analyze feature importance to gain insights into sentiment drivers\n",
    "\n",
    "This data preparation and modeling approach sets a solid foundation for our social media sentiment analysis project, combining efficiency with effectiveness to unlock valuable insights from vast amounts of text data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "798b98ab",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from stemmed_tweets.csv\n",
      "\n",
      "Sample of processed data:\n",
      "                                                text  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1  is upset that he can't update his Facebook by ...   \n",
      "2  @Kenichan I dived many times for the ball. Man...   \n",
      "3    my whole body feels itchy and like its on fire    \n",
      "4  @nationwideclass no, it's not behaving at all....   \n",
      "\n",
      "                                     stemmed_content  \n",
      "0  switchfoot http twitpic com zl awww bummer sho...  \n",
      "1  upset updat facebook text might cri result sch...  \n",
      "2  kenichan dive mani time ball manag save rest g...  \n",
      "3                    whole bodi feel itchi like fire  \n",
      "4                      nationwideclass behav mad see  \n",
      "\n",
      "Shape of the dataframe: (1600000, 8)\n",
      "\n",
      "Column names:\n",
      "['target', 'ids', 'date', 'flag', 'user', 'text', 'sentiment_score', 'stemmed_content']\n",
      "\n",
      "Number of null values in stemmed_content: 495\n",
      "\n",
      "Sample of rows with null values in stemmed_content:\n",
      "                  text stemmed_content\n",
      "3997       what to do              NaN\n",
      "4233             just              NaN\n",
      "18950        up again              NaN\n",
      "19062  I've been here              NaN\n",
      "24317   He's not here              NaN\n",
      "\n",
      "Unique values in 'text' column for rows with null stemmed_content:\n",
      "['what to do ' ' just ' 'up again ' \"I've been here \" \"He's not here \"\n",
      " 'and it did ' 'Its over! ' 'I did this to myself ' 'up at 4am again '\n",
      " '  over it' \"it's over \" ' i am so over it all.  ' \"There 10's. \"\n",
      " 'its 10:30 now ' \"No, I can't. \" 'Its over ' ' its over' 'not again '\n",
      " 'and again. ' 'why am i here? ' \"I'm over it \" 'is so 50/50 ' 'or not. '\n",
      " 'Where am I? ' '... if only ' 'AND there was no 49-O '\n",
      " 'Where are you??  .......' 'is just ' \"And we're up before 7. \"\n",
      " \"can't do this \" \"it's over now \" 'is down ' 'She is not there '\n",
      " \"... BUT I COULDN'T. \" 'i did it ' \"It's all over \" 'not on my own! '\n",
      " \"I just can't.... \" 'where is he? ' \"That'll be a 'no' then  \"\n",
      " '38... 10 more ' 'By myself ' 'is just so-so ' ' not again...'\n",
      " \" ... that's all\" 'what am i doing? what can i do? ' 'I only have 2 '\n",
      " 'What!?! ' 'I did that once ' '...just not that into me ' \"He's off! \"\n",
      " \"He's just not that into me \" \"It's over \" \"It's not over \" 'are on.. '\n",
      " 'All by myself. ' 'what did I do ' \"NO!!!! Don't do it! \"\n",
      " ' but then again ' 'Or not, then. ' \"I'm not in it \" \"Can't do this \"\n",
      " 'why?????............... ' ' so what then?' 'Where are you? '\n",
      " 'Why is it doing that? ' 'Re-doing it. ' 'is all by herself ' \"I'm too \"\n",
      " 'very very ' \"And. . . It's over \" \".....he's 19 \" 'where is him? '\n",
      " 'So am I ' 'All by myself ' \"It wasn't. \" ' what do I do?'\n",
      " 'UP..not in 3D ' 'down to $5.45 ' \"That's it.... \" 'should I? '\n",
      " 'or am i?  ' 'not me? ' \"I can't do this \" 'Now what?  '\n",
      " 'why me why me why me why me WHY ME!?!?! ' 'But why? ' 'No more up '\n",
      " 'What do I do now?? ' 'Why? Why? Why why why? Why? ' 'What should I do? '\n",
      " 'When will this be over!!!!! ' '  why me?' 'why again? ' 'where is he.. '\n",
      " \"i can't can't can't \" ' Where i' '@7_7  Why so?' 'ALL BY MYSELF '\n",
      " 'And... Up again.   ' 'Is  now.' '@the44s ' 'and i was there '\n",
      " '* :S not ' 'why!?  :S' \"He's off. \" 'Y am i up? ' 'Is up at 2 am '\n",
      " 'what to do??? ' 'Will ? ' 'Where is she... ' \"why aren't any of you on \"\n",
      " 'WHY ME  !' '...and we did ' 'Why why why ? ' 'its over ' 'is...... '\n",
      " 'WHAT DO I DO???????????????????????????? ' \"It's over now \"\n",
      " 'what about me?? ' 'up at 8 am ' 'why why why why why why why........ '\n",
      " 'down and out... ' 'WHAT???? ' \"if i don't;who will??? \"\n",
      " 'No. No, he does not. ' 'WHY IS IT NOT 8 '\n",
      " 'what am i to you, what are you to me? ' 'All by myself now '\n",
      " \"He's just not that into you \" '1-1 now ' 'Why He ? ' 'Me too '\n",
      " \"But I can't... \" ' its 335!' 'Where are you now? What are you doing? '\n",
      " 'If only.... If only... ' 'What will I do? ' 'where are you? '\n",
      " 'Is...                                                                 '\n",
      " \"I just can't.. \" 'up at 7 ' 'At UP but not 3D. '\n",
      " 'Why do I do this?!?!?!? ' \"It's over! \" 're too! '\n",
      " \"It's just not the same. \" ' why me?' 'Why is it over??? ' 'He+She= ;/, '\n",
      " 'who is?? me ' 'Down and Out ' 'why me ' 'What????? '\n",
      " 'its just not the same ' '    ...that is all.' 'just here ' '@t_will420 '\n",
      " 'what did i do ' \"I'm up by myself now. \" 'why..... ' 'where are you '\n",
      " '1 out of 3 ' 'Why me?!.... ' 'why do i do this to myself? '\n",
      " 'no..its not him ' 'Is any there ' 'Up at 3:41am.  Why?!?  ' 'No more! '\n",
      " 'They have... ' 'I only have $34 ' 'What??? ' \"I'm yours \"\n",
      " 'Where are you??  .' 'down and  out ' \"I'm out of here. \" 'Its 10:24 '\n",
      " 'Why!!!???  What I do??  ' 'me too ' '   Just... ' '3.0 where are you '\n",
      " 'just, down ' 'what can I do for them? ' 'Over it ' ' Not again...'\n",
      " 'WHY???   ' 'is in IT ' \"It's a 2:2 \" 'is now 22 ' ' .. just '\n",
      " 'What about me ' \"I'm so out of it. \" 'Or not ' 'Where you at. '\n",
      " 'what should i do ' 'what should I do? ' 'up at 6:30am '\n",
      " 'But what can you do. ' 'is it only 10:16 ' 'what to do now... '\n",
      " \"Haven't been this down down for a while. \" \"i'd be there \" 'Not Again '\n",
      " '....just ' 'and over again ' \"No He Doesn't \" 'what will i do now '\n",
      " ' over it' 'is up. ' 'why why why.... ' \"no it doesn't \" 'Is up now '\n",
      " 'WHERE IS HIM WHERE IS HIM WHERE IS HIM WHERE IS HIM WHERE IS HIM '\n",
      " 'I have to do it ' 'Why am i not over you? ' 'where are you?????? '\n",
      " \"but I'm not.  \" 'This is me... ' 'no on is on ' 'its only 2:40 '\n",
      " ' That is all.' 'Why do i do this?... ' \"Is I'll \"\n",
      " \"7:15am and i'm up. Not again. \" 'what did I do? ' \"There isn't \"\n",
      " 'Is there just me now ' 'I do not ' 'Why me ' 'all by myself. '\n",
      " '... or not ... ' ' why no can do?' \"and i don't have any $$ \"\n",
      " \"he's not \" \"....she's out \" \"I can't do it all \" \"I'm off! \" \"I can't  \"\n",
      " 'Why not? ' 'what should i do now ? ' ' what to do...' \"NO YOU CAN'T \"\n",
      " \"no it's not \" ' for what?' \"What're we doing?! \" 'How are you?  '\n",
      " 'Out and about ' 'Where am I?  ' '17 again ' 'is over it '\n",
      " 'now where is that ? ' 'is up and about! ' 'off for now '\n",
      " 'out from under ' ' (: which is which???' \"SO DON'T DO IT \"\n",
      " 'is up and about ' \"Now i'm not \" 'Out out out!!! ' 'This is it. '\n",
      " \"It's #420 am \" 'Why did i do that?  ' 'and it was ' 'We won  2-1'\n",
      " 'you and i ' 'What UP! ' \"That's what's up \" \"We're off \" 'is up!!  '\n",
      " 'He is...    ' 'We won !!!!!! ' 'Here I am! ' 'Who is this? ' 'It does! '\n",
      " '4:45am here ' \"i'm out...  \" 'me, here again! '\n",
      " 'we did what we had to do '\n",
      " \"There's a 1,000 you's... there's only 1 of me... \" 'Its 11:11 '\n",
      " 'is off ' 'what you doing? ' \"why wouldn't I? \" \"and we're in \"\n",
      " 'So over it! ' \"This is what it's all about \" 'Here again. ' 'so what ? '\n",
      " 'Just because... ' 'its not where you from, its where you at.. '\n",
      " 'We WON... ' 'Out and about! ' 'Its 1:15 am ' 'Just up '\n",
      " '17 Again _____  ' 'here again! ' 'this is me:   '\n",
      " 'What you up to then? ' 'You should ' 'You. Me. Now ' 'WE DID IT '\n",
      " \"i'm on again \" 'This is it! ' \"What's up ? \" 'Here again ' \"i'm up too \"\n",
      " 'Up and at them! ' 'I now have 11/50 ' \"...and I'm out!  \" 'Just me. '\n",
      " \"I'm OFF! \" 'Here on who. ' 'Up in 3-d ' 'to it!! ' 'I CAN DO THIS! '\n",
      " 'Up in 3d ' \"now you're there \" \"You don't! \" 'i did it. '\n",
      " 'I am not here! ' \"He's here \" 'Because you..... ' 'UP in 3D ' \"I'm out \"\n",
      " 'I am here $$ ' 'In the 42. ' 'Up in 3d = ' 'Now what?! '\n",
      " 'is herself again '\n",
      " '@yours12099 ???????????????????????????????????????????????????????????? '\n",
      " ' ...that is all' 'will do. ' 'Up and at-them ' 'It did! ' 'I am here! '\n",
      " '@302NOW ???? ?? ????? ' 'I do what i do for me not for you. '\n",
      " '................... What? ' 'about to re up ' \"I'm here again \"\n",
      " 'only you ' 'And there it is ' ' what it is? (:' 'Me too! ' 'Its 11 11 '\n",
      " \"...but then there's you \" '@Shan10 ' \"i'm off.  \" 'AGAIN AGAIN~ '\n",
      " 'So are we ' 'Up in 3D. ' \"I'm through \" '@ 4:13 now ' 'Off to be '\n",
      " 'OFF for now ' 'I Ã¢\\x99Â¥ you  ' 'BECAUSE OF YOU ' \" it's because of me!\"\n",
      " 'for now. ' \": It's OVER!!!! \" \"i'm OUT. \" \"And we're off \"\n",
      " '@yours12099 Ã¥Â¿Â«~! Ã§\\x94Â¨Ã¤Â½\\xa0Ã¥Â°?Ã¦\\x88Â²Ã¥\\x8a\\x87Ã¥?Â°Ã¨Â©\\x9eÃ§\\x9a\\x84Ã¨Â¶\\x85Ã¥Â¼Â·Ã¨Â¨\\x98Ã¦\\x86Â¶Ã¥\\x8a\\x9bÃ¯Â¼\\x8cÃ¦\\x8a\\x8aÃ¦\\x8b\\x9cÃ§\\x8aÂ¬Ã¦\\x9c\\x80Ã¥Â¾\\x8cÃ§\\x9a\\x84Ã©\\x82Â£Ã¤Â¸\\x80Ã¦Â®ÂµÃ¨Â©Â±Ã¥Â¯Â«Ã¤Â¸\\x8bÃ¤Â¾\\x86Ã¯Â¼\\x8cÃ¦\\x88\\x91Ã¨Â¦?Ã¦\\x89\\x93Ã¦\\x98\\x9fÃ¦\\x94Â¶Ã¨\\x97?Ã£\\x80\\x82 '\n",
      " 'It will ' 'and*  ' ' 23 more'\n",
      " 'Ã‘\\x85ÃÂ¾Ã‘\\x82Ã‘? @s13by ÃÂºÃÂ°ÃÂº Ã‘? ÃÂ²ÃÂ¸ÃÂ¶Ã‘\\x83. ÃÂ½ÃÂµ ÃÂ½ÃÂ°ÃÂ¿Ã‘\\x80Ã‘?ÃÂ³Ã‘?Ã‘? '\n",
      " 'You and me '\n",
      " '@s13by ÃÂ½ÃÂµ, ÃÂ¸ÃÂ·Ã‘\\x80ÃÂ°ÃÂ½ÃÂ¸ÃÂ» ÃÂ¼ÃÂµÃÂ½Ã‘? ÃÂ²Ã‘?ÃÂµÃÂ³ÃÂ¾, Ã‘?ÃÂ¸ÃÂ» ÃÂ¾Ã‘\\x82ÃÂ¾ÃÂ±Ã‘\\x80ÃÂ°ÃÂ» '\n",
      " 'is 18 now! ' \"'m doing, I have to do  \" 'and you? ' 'now... '\n",
      " 'because its all i have ' 'Once again ' 'just me '\n",
      " 'can you? 512 238 1905? ' \"in few...i'll be here!! \" 'off for now.... '\n",
      " 'with him ' 'out here. ' '@_in_ doing it now ' \"I'm OFF now! \"\n",
      " \"He's Just Not That Into You \" \"What's up? \" 'Up in 3D '\n",
      " \". . . . . and it's on! \"\n",
      " 'up ÃÂ¿ÃÂ¾ÃÂºÃÂ° ÃÂ²Ã‘\\x80ÃÂ¾ÃÂ´ÃÂµ ÃÂ²Ã‘?Ã‘\\x91 Ã‘\\x85ÃÂ¾Ã‘\\x80ÃÂ¾Ã‘\\x88ÃÂ¾ '\n",
      " 'or will they...??? ' 'Out of here ' 'out for now.. ' 'will do '\n",
      " '@302NOW Ã­\\x9e\\x98Ã«\\x82Â´Ã¬\\x84Â¸Ã¬\\x9a\\x94. ' 'what am I doing? '\n",
      " \"IT'S NOT ME..IT'S YOU \" 'You do now ' 'just am ' 'or not... '\n",
      " \"We're here \" 'Is here ' 'No just them and you. '\n",
      " 'what to where? what to where? ' 'Here I am ' 'up in 3d ' '@_who_is_she '\n",
      " 'Ã\\x91ÃÂµÃÂ·Ã‘\\x83ÃÂ¼ÃÂ½ÃÂµÃÂ½Ã‘\\x8cÃÂºÃÂ°Ã‘? ÃÂ½ÃÂ¾Ã‘\\x87ÃÂºÃÂ°. Ã\\x94ÃÂ°, ÃÂ½ÃÂµ ÃÂ¾ÃÂ¶ÃÂ¸ÃÂ´ÃÂ°ÃÂ» ÃÂ¾Ã‘\\x82 Ã‘?ÃÂµÃÂ±Ã‘? Ã‘\\x82ÃÂ°ÃÂºÃÂ¾ÃÂ³ÃÂ¾ Ã‘\\x80ÃÂµÃÂ·ÃÂ²ÃÂ¾ÃÂ³ÃÂ¾ Ã‘?Ã‘\\x82ÃÂ°ÃÂ¹ÃÂ»ÃÂ¸ÃÂºÃÂ°  Ã\\x94ÃÂ¾ÃÂ±Ã‘\\x80ÃÂ¾ÃÂµ Ã‘\\x83Ã‘\\x82Ã‘\\x80ÃÂ¾, Ã‘?Ã‘\\x83ÃÂ±ÃÂ±ÃÂ¾Ã‘\\x82ÃÂºÃÂ°.  Here I am !'\n",
      " 'off now ' '@LL50220 Will do. ' 'now what do I do '\n",
      " 'Up and at it...again! ' 'I am what I am '\n",
      " \"I'm only me when i'm with you! \" 'Were 4/4 ' 'is out! ' 'Is that me? '\n",
      " \"He's just not that in to me \" 'Off!!! ' 'and off '\n",
      " '@302NOW Ã¬\\x9e\\x98 Ã«\\x8bÂ¤Ã«\\x85\\x80 Ã¬\\x98Â¤Ã¬\\x84Â¸Ã¬\\x9a\\x94~ '\n",
      " \"And i'm only me when i'm with you \" \"but when that's over.... \"\n",
      " 'We so are ' 'Is now 16 ' 'Here i am ' \"But I didn't. \" 'So what !  '\n",
      " \"I'm off. \" 'Over and out! ' \"It's only 2:30 \" 'ItÃ‚Â´s all about you! '\n",
      " 'all to myself ' 'SHE WON! ' 'just had a few. ' '......... now. '\n",
      " 'up and about! ' \" what's that about?\" 'Just Be Yourself ' 'm off... '\n",
      " 'here i am again ' \"what's up? \" \"what's up \" \"I'm doing that i'm doing \"\n",
      " 'Out and About ' \"Now, i'm on.  What's up? \" \"I'm on the UP and up \"\n",
      " 'Up!Up!Up! ' 'i have it ' 'we did it! ' 'That is all. ' 'Up and at it '\n",
      " 'Here they are. ' 'here i am! ' 'so this it .. then ' 'here I am '\n",
      " 'is ON!!!!!! ' 'If only? ' 'to be with you ']\n",
      "\n",
      "Number of null values in stemmed_content after handling: 0\n",
      "\n",
      "Memory usage of the dataframe: 661.18 MB\n",
      "\n",
      "Shape of vectorized training data: (1280016, 5000)\n",
      "Shape of vectorized testing data: (160000, 5000)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "def load_processed_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading processed data from {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Processed data file not found: {file_path}\")\n",
    "\n",
    "processed_file = 'stemmed_tweets.csv'\n",
    "df = load_processed_data(processed_file)\n",
    "\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(df[['text', 'stemmed_content']].head())\n",
    "\n",
    "print(f\"\\nShape of the dataframe: {df.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "null_count = df['stemmed_content'].isnull().sum()\n",
    "print(f\"\\nNumber of null values in stemmed_content: {null_count}\")\n",
    "\n",
    "if null_count > 0:\n",
    "    print(\"\\nSample of rows with null values in stemmed_content:\")\n",
    "    print(df[df['stemmed_content'].isnull()][['text', 'stemmed_content']].head())\n",
    "    \n",
    "    print(\"\\nUnique values in 'text' column for rows with null stemmed_content:\")\n",
    "    print(df[df['stemmed_content'].isnull()]['text'].unique())\n",
    "\n",
    "df['stemmed_content'] = df['stemmed_content'].fillna('')\n",
    "\n",
    "null_count_after = df['stemmed_content'].isnull().sum()\n",
    "print(f\"\\nNumber of null values in stemmed_content after handling: {null_count_after}\")\n",
    "\n",
    "memory_usage = df.memory_usage(deep=True).sum() / 1024**2  # in MB\n",
    "print(f\"\\nMemory usage of the dataframe: {memory_usage:.2f} MB\")\n",
    "\n",
    "X = df['stemmed_content']\n",
    "y = df['target']  # Using 'target' as the label column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1111, random_state=42)\n",
    "\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_vectorized = vectorizer.fit_transform(X_train)\n",
    "X_test_vectorized = vectorizer.transform(X_test)\n",
    "print(\"\\nShape of vectorized training data:\", X_train_vectorized.shape)\n",
    "print(\"Shape of vectorized testing data:\", X_test_vectorized.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c365ba10",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from stemmed_tweets.csv\n",
      "\n",
      "Sample of processed data:\n",
      "                                                text  \\\n",
      "0  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
      "1  is upset that he can't update his Facebook by ...   \n",
      "2  @Kenichan I dived many times for the ball. Man...   \n",
      "3    my whole body feels itchy and like its on fire    \n",
      "4  @nationwideclass no, it's not behaving at all....   \n",
      "\n",
      "                                     stemmed_content  \n",
      "0  switchfoot http twitpic com zl awww bummer sho...  \n",
      "1  upset updat facebook text might cri result sch...  \n",
      "2  kenichan dive mani time ball manag save rest g...  \n",
      "3                    whole bodi feel itchi like fire  \n",
      "4                      nationwideclass behav mad see  \n",
      "\n",
      "Shape of the dataframe: (1600000, 8)\n",
      "\n",
      "Column names:\n",
      "['target', 'ids', 'date', 'flag', 'user', 'text', 'sentiment_score', 'stemmed_content']\n",
      "\n",
      "Number of null values in stemmed_content: 495\n",
      "\n",
      "Sample of rows with null values in stemmed_content:\n",
      "                  text stemmed_content\n",
      "3997       what to do              NaN\n",
      "4233             just              NaN\n",
      "18950        up again              NaN\n",
      "19062  I've been here              NaN\n",
      "24317   He's not here              NaN\n",
      "\n",
      "Unique values in 'text' column for rows with null stemmed_content:\n",
      "['what to do ' ' just ' 'up again ' \"I've been here \" \"He's not here \"\n",
      " 'and it did ' 'Its over! ' 'I did this to myself ' 'up at 4am again '\n",
      " '  over it' \"it's over \" ' i am so over it all.  ' \"There 10's. \"\n",
      " 'its 10:30 now ' \"No, I can't. \" 'Its over ' ' its over' 'not again '\n",
      " 'and again. ' 'why am i here? ' \"I'm over it \" 'is so 50/50 ' 'or not. '\n",
      " 'Where am I? ' '... if only ' 'AND there was no 49-O '\n",
      " 'Where are you??  .......' 'is just ' \"And we're up before 7. \"\n",
      " \"can't do this \" \"it's over now \" 'is down ' 'She is not there '\n",
      " \"... BUT I COULDN'T. \" 'i did it ' \"It's all over \" 'not on my own! '\n",
      " \"I just can't.... \" 'where is he? ' \"That'll be a 'no' then  \"\n",
      " '38... 10 more ' 'By myself ' 'is just so-so ' ' not again...'\n",
      " \" ... that's all\" 'what am i doing? what can i do? ' 'I only have 2 '\n",
      " 'What!?! ' 'I did that once ' '...just not that into me ' \"He's off! \"\n",
      " \"He's just not that into me \" \"It's over \" \"It's not over \" 'are on.. '\n",
      " 'All by myself. ' 'what did I do ' \"NO!!!! Don't do it! \"\n",
      " ' but then again ' 'Or not, then. ' \"I'm not in it \" \"Can't do this \"\n",
      " 'why?????............... ' ' so what then?' 'Where are you? '\n",
      " 'Why is it doing that? ' 'Re-doing it. ' 'is all by herself ' \"I'm too \"\n",
      " 'very very ' \"And. . . It's over \" \".....he's 19 \" 'where is him? '\n",
      " 'So am I ' 'All by myself ' \"It wasn't. \" ' what do I do?'\n",
      " 'UP..not in 3D ' 'down to $5.45 ' \"That's it.... \" 'should I? '\n",
      " 'or am i?  ' 'not me? ' \"I can't do this \" 'Now what?  '\n",
      " 'why me why me why me why me WHY ME!?!?! ' 'But why? ' 'No more up '\n",
      " 'What do I do now?? ' 'Why? Why? Why why why? Why? ' 'What should I do? '\n",
      " 'When will this be over!!!!! ' '  why me?' 'why again? ' 'where is he.. '\n",
      " \"i can't can't can't \" ' Where i' '@7_7  Why so?' 'ALL BY MYSELF '\n",
      " 'And... Up again.   ' 'Is  now.' '@the44s ' 'and i was there '\n",
      " '* :S not ' 'why!?  :S' \"He's off. \" 'Y am i up? ' 'Is up at 2 am '\n",
      " 'what to do??? ' 'Will ? ' 'Where is she... ' \"why aren't any of you on \"\n",
      " 'WHY ME  !' '...and we did ' 'Why why why ? ' 'its over ' 'is...... '\n",
      " 'WHAT DO I DO???????????????????????????? ' \"It's over now \"\n",
      " 'what about me?? ' 'up at 8 am ' 'why why why why why why why........ '\n",
      " 'down and out... ' 'WHAT???? ' \"if i don't;who will??? \"\n",
      " 'No. No, he does not. ' 'WHY IS IT NOT 8 '\n",
      " 'what am i to you, what are you to me? ' 'All by myself now '\n",
      " \"He's just not that into you \" '1-1 now ' 'Why He ? ' 'Me too '\n",
      " \"But I can't... \" ' its 335!' 'Where are you now? What are you doing? '\n",
      " 'If only.... If only... ' 'What will I do? ' 'where are you? '\n",
      " 'Is...                                                                 '\n",
      " \"I just can't.. \" 'up at 7 ' 'At UP but not 3D. '\n",
      " 'Why do I do this?!?!?!? ' \"It's over! \" 're too! '\n",
      " \"It's just not the same. \" ' why me?' 'Why is it over??? ' 'He+She= ;/, '\n",
      " 'who is?? me ' 'Down and Out ' 'why me ' 'What????? '\n",
      " 'its just not the same ' '    ...that is all.' 'just here ' '@t_will420 '\n",
      " 'what did i do ' \"I'm up by myself now. \" 'why..... ' 'where are you '\n",
      " '1 out of 3 ' 'Why me?!.... ' 'why do i do this to myself? '\n",
      " 'no..its not him ' 'Is any there ' 'Up at 3:41am.  Why?!?  ' 'No more! '\n",
      " 'They have... ' 'I only have $34 ' 'What??? ' \"I'm yours \"\n",
      " 'Where are you??  .' 'down and  out ' \"I'm out of here. \" 'Its 10:24 '\n",
      " 'Why!!!???  What I do??  ' 'me too ' '   Just... ' '3.0 where are you '\n",
      " 'just, down ' 'what can I do for them? ' 'Over it ' ' Not again...'\n",
      " 'WHY???   ' 'is in IT ' \"It's a 2:2 \" 'is now 22 ' ' .. just '\n",
      " 'What about me ' \"I'm so out of it. \" 'Or not ' 'Where you at. '\n",
      " 'what should i do ' 'what should I do? ' 'up at 6:30am '\n",
      " 'But what can you do. ' 'is it only 10:16 ' 'what to do now... '\n",
      " \"Haven't been this down down for a while. \" \"i'd be there \" 'Not Again '\n",
      " '....just ' 'and over again ' \"No He Doesn't \" 'what will i do now '\n",
      " ' over it' 'is up. ' 'why why why.... ' \"no it doesn't \" 'Is up now '\n",
      " 'WHERE IS HIM WHERE IS HIM WHERE IS HIM WHERE IS HIM WHERE IS HIM '\n",
      " 'I have to do it ' 'Why am i not over you? ' 'where are you?????? '\n",
      " \"but I'm not.  \" 'This is me... ' 'no on is on ' 'its only 2:40 '\n",
      " ' That is all.' 'Why do i do this?... ' \"Is I'll \"\n",
      " \"7:15am and i'm up. Not again. \" 'what did I do? ' \"There isn't \"\n",
      " 'Is there just me now ' 'I do not ' 'Why me ' 'all by myself. '\n",
      " '... or not ... ' ' why no can do?' \"and i don't have any $$ \"\n",
      " \"he's not \" \"....she's out \" \"I can't do it all \" \"I'm off! \" \"I can't  \"\n",
      " 'Why not? ' 'what should i do now ? ' ' what to do...' \"NO YOU CAN'T \"\n",
      " \"no it's not \" ' for what?' \"What're we doing?! \" 'How are you?  '\n",
      " 'Out and about ' 'Where am I?  ' '17 again ' 'is over it '\n",
      " 'now where is that ? ' 'is up and about! ' 'off for now '\n",
      " 'out from under ' ' (: which is which???' \"SO DON'T DO IT \"\n",
      " 'is up and about ' \"Now i'm not \" 'Out out out!!! ' 'This is it. '\n",
      " \"It's #420 am \" 'Why did i do that?  ' 'and it was ' 'We won  2-1'\n",
      " 'you and i ' 'What UP! ' \"That's what's up \" \"We're off \" 'is up!!  '\n",
      " 'He is...    ' 'We won !!!!!! ' 'Here I am! ' 'Who is this? ' 'It does! '\n",
      " '4:45am here ' \"i'm out...  \" 'me, here again! '\n",
      " 'we did what we had to do '\n",
      " \"There's a 1,000 you's... there's only 1 of me... \" 'Its 11:11 '\n",
      " 'is off ' 'what you doing? ' \"why wouldn't I? \" \"and we're in \"\n",
      " 'So over it! ' \"This is what it's all about \" 'Here again. ' 'so what ? '\n",
      " 'Just because... ' 'its not where you from, its where you at.. '\n",
      " 'We WON... ' 'Out and about! ' 'Its 1:15 am ' 'Just up '\n",
      " '17 Again _____  ' 'here again! ' 'this is me:   '\n",
      " 'What you up to then? ' 'You should ' 'You. Me. Now ' 'WE DID IT '\n",
      " \"i'm on again \" 'This is it! ' \"What's up ? \" 'Here again ' \"i'm up too \"\n",
      " 'Up and at them! ' 'I now have 11/50 ' \"...and I'm out!  \" 'Just me. '\n",
      " \"I'm OFF! \" 'Here on who. ' 'Up in 3-d ' 'to it!! ' 'I CAN DO THIS! '\n",
      " 'Up in 3d ' \"now you're there \" \"You don't! \" 'i did it. '\n",
      " 'I am not here! ' \"He's here \" 'Because you..... ' 'UP in 3D ' \"I'm out \"\n",
      " 'I am here $$ ' 'In the 42. ' 'Up in 3d = ' 'Now what?! '\n",
      " 'is herself again '\n",
      " '@yours12099 ???????????????????????????????????????????????????????????? '\n",
      " ' ...that is all' 'will do. ' 'Up and at-them ' 'It did! ' 'I am here! '\n",
      " '@302NOW ???? ?? ????? ' 'I do what i do for me not for you. '\n",
      " '................... What? ' 'about to re up ' \"I'm here again \"\n",
      " 'only you ' 'And there it is ' ' what it is? (:' 'Me too! ' 'Its 11 11 '\n",
      " \"...but then there's you \" '@Shan10 ' \"i'm off.  \" 'AGAIN AGAIN~ '\n",
      " 'So are we ' 'Up in 3D. ' \"I'm through \" '@ 4:13 now ' 'Off to be '\n",
      " 'OFF for now ' 'I Ã¢\\x99Â¥ you  ' 'BECAUSE OF YOU ' \" it's because of me!\"\n",
      " 'for now. ' \": It's OVER!!!! \" \"i'm OUT. \" \"And we're off \"\n",
      " '@yours12099 Ã¥Â¿Â«~! Ã§\\x94Â¨Ã¤Â½\\xa0Ã¥Â°?Ã¦\\x88Â²Ã¥\\x8a\\x87Ã¥?Â°Ã¨Â©\\x9eÃ§\\x9a\\x84Ã¨Â¶\\x85Ã¥Â¼Â·Ã¨Â¨\\x98Ã¦\\x86Â¶Ã¥\\x8a\\x9bÃ¯Â¼\\x8cÃ¦\\x8a\\x8aÃ¦\\x8b\\x9cÃ§\\x8aÂ¬Ã¦\\x9c\\x80Ã¥Â¾\\x8cÃ§\\x9a\\x84Ã©\\x82Â£Ã¤Â¸\\x80Ã¦Â®ÂµÃ¨Â©Â±Ã¥Â¯Â«Ã¤Â¸\\x8bÃ¤Â¾\\x86Ã¯Â¼\\x8cÃ¦\\x88\\x91Ã¨Â¦?Ã¦\\x89\\x93Ã¦\\x98\\x9fÃ¦\\x94Â¶Ã¨\\x97?Ã£\\x80\\x82 '\n",
      " 'It will ' 'and*  ' ' 23 more'\n",
      " 'Ã‘\\x85ÃÂ¾Ã‘\\x82Ã‘? @s13by ÃÂºÃÂ°ÃÂº Ã‘? ÃÂ²ÃÂ¸ÃÂ¶Ã‘\\x83. ÃÂ½ÃÂµ ÃÂ½ÃÂ°ÃÂ¿Ã‘\\x80Ã‘?ÃÂ³Ã‘?Ã‘? '\n",
      " 'You and me '\n",
      " '@s13by ÃÂ½ÃÂµ, ÃÂ¸ÃÂ·Ã‘\\x80ÃÂ°ÃÂ½ÃÂ¸ÃÂ» ÃÂ¼ÃÂµÃÂ½Ã‘? ÃÂ²Ã‘?ÃÂµÃÂ³ÃÂ¾, Ã‘?ÃÂ¸ÃÂ» ÃÂ¾Ã‘\\x82ÃÂ¾ÃÂ±Ã‘\\x80ÃÂ°ÃÂ» '\n",
      " 'is 18 now! ' \"'m doing, I have to do  \" 'and you? ' 'now... '\n",
      " 'because its all i have ' 'Once again ' 'just me '\n",
      " 'can you? 512 238 1905? ' \"in few...i'll be here!! \" 'off for now.... '\n",
      " 'with him ' 'out here. ' '@_in_ doing it now ' \"I'm OFF now! \"\n",
      " \"He's Just Not That Into You \" \"What's up? \" 'Up in 3D '\n",
      " \". . . . . and it's on! \"\n",
      " 'up ÃÂ¿ÃÂ¾ÃÂºÃÂ° ÃÂ²Ã‘\\x80ÃÂ¾ÃÂ´ÃÂµ ÃÂ²Ã‘?Ã‘\\x91 Ã‘\\x85ÃÂ¾Ã‘\\x80ÃÂ¾Ã‘\\x88ÃÂ¾ '\n",
      " 'or will they...??? ' 'Out of here ' 'out for now.. ' 'will do '\n",
      " '@302NOW Ã­\\x9e\\x98Ã«\\x82Â´Ã¬\\x84Â¸Ã¬\\x9a\\x94. ' 'what am I doing? '\n",
      " \"IT'S NOT ME..IT'S YOU \" 'You do now ' 'just am ' 'or not... '\n",
      " \"We're here \" 'Is here ' 'No just them and you. '\n",
      " 'what to where? what to where? ' 'Here I am ' 'up in 3d ' '@_who_is_she '\n",
      " 'Ã\\x91ÃÂµÃÂ·Ã‘\\x83ÃÂ¼ÃÂ½ÃÂµÃÂ½Ã‘\\x8cÃÂºÃÂ°Ã‘? ÃÂ½ÃÂ¾Ã‘\\x87ÃÂºÃÂ°. Ã\\x94ÃÂ°, ÃÂ½ÃÂµ ÃÂ¾ÃÂ¶ÃÂ¸ÃÂ´ÃÂ°ÃÂ» ÃÂ¾Ã‘\\x82 Ã‘?ÃÂµÃÂ±Ã‘? Ã‘\\x82ÃÂ°ÃÂºÃÂ¾ÃÂ³ÃÂ¾ Ã‘\\x80ÃÂµÃÂ·ÃÂ²ÃÂ¾ÃÂ³ÃÂ¾ Ã‘?Ã‘\\x82ÃÂ°ÃÂ¹ÃÂ»ÃÂ¸ÃÂºÃÂ°  Ã\\x94ÃÂ¾ÃÂ±Ã‘\\x80ÃÂ¾ÃÂµ Ã‘\\x83Ã‘\\x82Ã‘\\x80ÃÂ¾, Ã‘?Ã‘\\x83ÃÂ±ÃÂ±ÃÂ¾Ã‘\\x82ÃÂºÃÂ°.  Here I am !'\n",
      " 'off now ' '@LL50220 Will do. ' 'now what do I do '\n",
      " 'Up and at it...again! ' 'I am what I am '\n",
      " \"I'm only me when i'm with you! \" 'Were 4/4 ' 'is out! ' 'Is that me? '\n",
      " \"He's just not that in to me \" 'Off!!! ' 'and off '\n",
      " '@302NOW Ã¬\\x9e\\x98 Ã«\\x8bÂ¤Ã«\\x85\\x80 Ã¬\\x98Â¤Ã¬\\x84Â¸Ã¬\\x9a\\x94~ '\n",
      " \"And i'm only me when i'm with you \" \"but when that's over.... \"\n",
      " 'We so are ' 'Is now 16 ' 'Here i am ' \"But I didn't. \" 'So what !  '\n",
      " \"I'm off. \" 'Over and out! ' \"It's only 2:30 \" 'ItÃ‚Â´s all about you! '\n",
      " 'all to myself ' 'SHE WON! ' 'just had a few. ' '......... now. '\n",
      " 'up and about! ' \" what's that about?\" 'Just Be Yourself ' 'm off... '\n",
      " 'here i am again ' \"what's up? \" \"what's up \" \"I'm doing that i'm doing \"\n",
      " 'Out and About ' \"Now, i'm on.  What's up? \" \"I'm on the UP and up \"\n",
      " 'Up!Up!Up! ' 'i have it ' 'we did it! ' 'That is all. ' 'Up and at it '\n",
      " 'Here they are. ' 'here i am! ' 'so this it .. then ' 'here I am '\n",
      " 'is ON!!!!!! ' 'If only? ' 'to be with you ']\n",
      "\n",
      "Number of null values in stemmed_content after handling: 0\n",
      "\n",
      "Memory usage of the dataframe: 661.18 MB\n",
      "\n",
      "Shape of vectorized training data: (1280016, 5000)\n",
      "Shape of vectorized testing data: (160000, 5000)\n",
      "Logistic Regression model trained successfully.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yaswa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "def load_processed_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading processed data from {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Processed data file not found: {file_path}\")\n",
    "\n",
    "processed_file = 'stemmed_tweets.csv'\n",
    "df = load_processed_data(processed_file)\n",
    "\n",
    "print(\"\\nSample of processed data:\")\n",
    "print(df[['text', 'stemmed_content']].head())\n",
    "\n",
    "print(f\"\\nShape of the dataframe: {df.shape}\")\n",
    "print(\"\\nColumn names:\")\n",
    "print(df.columns.tolist())\n",
    "\n",
    "null_count = df['stemmed_content'].isnull().sum()\n",
    "print(f\"\\nNumber of null values in stemmed_content: {null_count}\")\n",
    "\n",
    "if null_count > 0:\n",
    "    print(\"\\nSample of rows with null values in stemmed_content:\")\n",
    "    print(df[df['stemmed_content'].isnull()][['text', 'stemmed_content']].head())\n",
    "    \n",
    "    print(\"\\nUnique values in 'text' column for rows with null stemmed_content:\")\n",
    "    print(df[df['stemmed_content'].isnull()]['text'].unique())\n",
    "\n",
    "df['stemmed_content'] = df['stemmed_content'].fillna('')\n",
    "\n",
    "null_count_after = df['stemmed_content'].isnull().sum()\n",
    "print(f\"\\nNumber of null values in stemmed_content after handling: {null_count_after}\")\n",
    "\n",
    "memory_usage = df.memory_usage(deep=True).sum() / 1024**2  # in MB\n",
    "print(f\"\\nMemory usage of the dataframe: {memory_usage:.2f} MB\")\n",
    "\n",
    "X = df[['stemmed_content', 'sentiment_score']]\n",
    "y = df['target']  # Using 'target' as the label column\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=42)\n",
    "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.1111, random_state=42)\n",
    "\n",
    "# Initialize and fit the vectorizer for the text data\n",
    "vectorizer = TfidfVectorizer(max_features=5000)\n",
    "X_train_text_vectorized = vectorizer.fit_transform(X_train['stemmed_content'])\n",
    "X_test_text_vectorized = vectorizer.transform(X_test['stemmed_content'])\n",
    "\n",
    "print(\"\\nShape of vectorized training data:\", X_train_text_vectorized.shape)\n",
    "print(\"Shape of vectorized testing data:\", X_test_text_vectorized.shape)\n",
    "\n",
    "import scipy.sparse as sp\n",
    "\n",
    "X_train_combined = sp.hstack((X_train_text_vectorized, X_train[['sentiment_score']].values))\n",
    "X_test_combined = sp.hstack((X_test_text_vectorized, X_test[['sentiment_score']].values))\n",
    "\n",
    "log = LogisticRegression()\n",
    "log.fit(X_train_combined, y_train)\n",
    "\n",
    "print(\"Logistic Regression model trained successfully.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "b8a8c314",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>flag</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>stemmed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>-0.0173</td>\n",
       "      <td>switchfoot http twitpic com zl awww bummer sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>-0.7500</td>\n",
       "      <td>upset updat facebook text might cri result sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>kenichan dive mani time ball manag save rest g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>nationwideclass behav mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599995</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>woke school best feel ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599996</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>0.4376</td>\n",
       "      <td>thewdb com cool hear old walt interview http b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599997</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>readi mojo makeov ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599998</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>0.6784</td>\n",
       "      <td>happi th birthday boo alll time tupac amaru sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599999</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>NO_QUERY</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>happi charitytuesday thenspcc sparkschar speak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1600000 rows Ã— 8 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date      flag  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  NO_QUERY   \n",
       "1            -1  1467810672  Mon Apr 06 22:19:49 PDT 2009  NO_QUERY   \n",
       "2            -1  1467810917  Mon Apr 06 22:19:53 PDT 2009  NO_QUERY   \n",
       "3            -1  1467811184  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "4            -1  1467811193  Mon Apr 06 22:19:57 PDT 2009  NO_QUERY   \n",
       "...         ...         ...                           ...       ...   \n",
       "1599995       1  2193601966  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599996       1  2193601969  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599997       1  2193601991  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599998       1  2193602064  Tue Jun 16 08:40:49 PDT 2009  NO_QUERY   \n",
       "1599999       1  2193602129  Tue Jun 16 08:40:50 PDT 2009  NO_QUERY   \n",
       "\n",
       "                    user                                               text  \\\n",
       "0        _TheSpecialOne_  @switchfoot http://twitpic.com/2y1zl - Awww, t...   \n",
       "1          scotthamilton  is upset that he can't update his Facebook by ...   \n",
       "2               mattycus  @Kenichan I dived many times for the ball. Man...   \n",
       "3                ElleCTF    my whole body feels itchy and like its on fire    \n",
       "4                 Karoli  @nationwideclass no, it's not behaving at all....   \n",
       "...                  ...                                                ...   \n",
       "1599995  AmandaMarie1028  Just woke up. Having no school is the best fee...   \n",
       "1599996      TheWDBoards  TheWDB.com - Very cool to hear old Walt interv...   \n",
       "1599997           bpbabe  Are you ready for your MoJo Makeover? Ask me f...   \n",
       "1599998     tinydiamondz  Happy 38th Birthday to my boo of alll time!!! ...   \n",
       "1599999   RyanTrevMorris  happy #charitytuesday @theNSPCC @SparksCharity...   \n",
       "\n",
       "         sentiment_score                                    stemmed_content  \n",
       "0                -0.0173  switchfoot http twitpic com zl awww bummer sho...  \n",
       "1                -0.7500  upset updat facebook text might cri result sch...  \n",
       "2                 0.4939  kenichan dive mani time ball manag save rest g...  \n",
       "3                -0.2500                    whole bodi feel itchi like fire  \n",
       "4                -0.6597                      nationwideclass behav mad see  \n",
       "...                  ...                                                ...  \n",
       "1599995           0.5423                         woke school best feel ever  \n",
       "1599996           0.4376  thewdb com cool hear old walt interview http b...  \n",
       "1599997           0.3612                       readi mojo makeov ask detail  \n",
       "1599998           0.6784  happi th birthday boo alll time tupac amaru sh...  \n",
       "1599999           0.5719  happi charitytuesday thenspcc sparkschar speak...  \n",
       "\n",
       "[1600000 rows x 8 columns]"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "7d97ce3a-d5b1-4f37-a58d-c85e2462e771",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: klib in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (1.3.1)\n",
      "Requirement already satisfied: Jinja2<4.0.0,>=3.1.0 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from klib) (3.1.3)\n",
      "Requirement already satisfied: matplotlib<4.0.0,>=3.6.0 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from klib) (3.8.3)\n",
      "Requirement already satisfied: numpy<2.0.0,>=1.26.0 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from klib) (1.26.4)\n",
      "Requirement already satisfied: pandas<3.0,>=1.4 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from klib) (2.2.1)\n",
      "Requirement already satisfied: plotly<6.0.0,>=5.11.0 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from klib) (5.23.0)\n",
      "Requirement already satisfied: scipy<2.0.0,>=1.10.0 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from klib) (1.12.0)\n",
      "Requirement already satisfied: screeninfo<0.9.0,>=0.8.1 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from klib) (0.8.1)\n",
      "Requirement already satisfied: seaborn>=0.12.0 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from klib) (0.13.2)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from Jinja2<4.0.0,>=3.1.0->klib) (2.1.5)\n",
      "Requirement already satisfied: contourpy>=1.0.1 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<4.0.0,>=3.6.0->klib) (1.2.0)\n",
      "Requirement already satisfied: cycler>=0.10 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<4.0.0,>=3.6.0->klib) (0.12.1)\n",
      "Requirement already satisfied: fonttools>=4.22.0 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<4.0.0,>=3.6.0->klib) (4.49.0)\n",
      "Requirement already satisfied: kiwisolver>=1.3.1 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<4.0.0,>=3.6.0->klib) (1.4.5)\n",
      "Requirement already satisfied: packaging>=20.0 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<4.0.0,>=3.6.0->klib) (23.2)\n",
      "Requirement already satisfied: pillow>=8 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<4.0.0,>=3.6.0->klib) (10.2.0)\n",
      "Requirement already satisfied: pyparsing>=2.3.1 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<4.0.0,>=3.6.0->klib) (3.1.1)\n",
      "Requirement already satisfied: python-dateutil>=2.7 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from matplotlib<4.0.0,>=3.6.0->klib) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3.0,>=1.4->klib) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from pandas<3.0,>=1.4->klib) (2024.1)\n",
      "Requirement already satisfied: tenacity>=6.2.0 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from plotly<6.0.0,>=5.11.0->klib) (9.0.0)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\yaswa\\appdata\\local\\programs\\python\\python312\\lib\\site-packages (from python-dateutil>=2.7->matplotlib<4.0.0,>=3.6.0->klib) (1.16.0)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.2\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install klib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "3ce911b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import klib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "76c9d43e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of cleaned data: (1599693, 7) - Remaining NAs: 0\n",
      "\n",
      "\n",
      "Dropped rows: 307\n",
      "     of which 307 duplicates. (Rows (first 150 shown): [801280, 804316, 804656, 804929, 809639, 823566, 823595, 823619, 830626, 830972, 833961, 834508, 840266, 841026, 842068, 842479, 842826, 846865, 847570, 851371, 865056, 865716, 870364, 870367, 874372, 875794, 878756, 878853, 880198, 881514, 881861, 882949, 883653, 889465, 890486, 891462, 903500, 904559, 905072, 907905, 911334, 912551, 914199, 915818, 920187, 924751, 924878, 925756, 927257, 927648, 934067, 937070, 938462, 941125, 942203, 943043, 944326, 948418, 950858, 954263, 955475, 956465, 956776, 959366, 965343, 966748, 967536, 970029, 972173, 977743, 981152, 981548, 983781, 986244, 987152, 988945, 989829, 999959, 1006913, 1007046, 1008577, 1017840, 1018342, 1018882, 1019074, 1025446, 1027931, 1032965, 1034890, 1034955, 1041568, 1042877, 1045534, 1047232, 1057137, 1058328, 1060413, 1060468, 1060622, 1064346, 1067971, 1074156, 1080867, 1081552, 1084605, 1085190, 1086997, 1088175, 1088899, 1095083, 1095614, 1100577, 1100711, 1102820, 1102843, 1104125, 1106867, 1111223, 1111794, 1117973, 1120550, 1124430, 1128251, 1130204, 1132707, 1134933, 1135767, 1137474, 1138509, 1139927, 1140067, 1149132, 1149594, 1152371, 1163023, 1165114, 1165546, 1167629, 1176539, 1180809, 1182084, 1182149, 1182206, 1182392, 1183700, 1187551, 1188417, 1189429, 1197080, 1197182])\n",
      "\n",
      "Dropped columns: 1\n",
      "     of which 1 single valued.     Columns: ['flag']\n",
      "Dropped missing values: 0\n",
      "Reduced memory by at least: 29.01 MB (-29.71%)\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>target</th>\n",
       "      <th>ids</th>\n",
       "      <th>date</th>\n",
       "      <th>user</th>\n",
       "      <th>text</th>\n",
       "      <th>sentiment_score</th>\n",
       "      <th>stemmed_content</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>1467810369</td>\n",
       "      <td>Mon Apr 06 22:19:45 PDT 2009</td>\n",
       "      <td>_TheSpecialOne_</td>\n",
       "      <td>@switchfoot http://twitpic.com/2y1zl - Awww, t...</td>\n",
       "      <td>-0.0173</td>\n",
       "      <td>switchfoot http twitpic com zl awww bummer sho...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467810672</td>\n",
       "      <td>Mon Apr 06 22:19:49 PDT 2009</td>\n",
       "      <td>scotthamilton</td>\n",
       "      <td>is upset that he can't update his Facebook by ...</td>\n",
       "      <td>-0.7500</td>\n",
       "      <td>upset updat facebook text might cri result sch...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467810917</td>\n",
       "      <td>Mon Apr 06 22:19:53 PDT 2009</td>\n",
       "      <td>mattycus</td>\n",
       "      <td>@Kenichan I dived many times for the ball. Man...</td>\n",
       "      <td>0.4939</td>\n",
       "      <td>kenichan dive mani time ball manag save rest g...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467811184</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>ElleCTF</td>\n",
       "      <td>my whole body feels itchy and like its on fire</td>\n",
       "      <td>-0.2500</td>\n",
       "      <td>whole bodi feel itchi like fire</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-1</td>\n",
       "      <td>1467811193</td>\n",
       "      <td>Mon Apr 06 22:19:57 PDT 2009</td>\n",
       "      <td>Karoli</td>\n",
       "      <td>@nationwideclass no, it's not behaving at all....</td>\n",
       "      <td>-0.6597</td>\n",
       "      <td>nationwideclass behav mad see</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599688</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601966</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>AmandaMarie1028</td>\n",
       "      <td>Just woke up. Having no school is the best fee...</td>\n",
       "      <td>0.5423</td>\n",
       "      <td>woke school best feel ever</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599689</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601969</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>TheWDBoards</td>\n",
       "      <td>TheWDB.com - Very cool to hear old Walt interv...</td>\n",
       "      <td>0.4376</td>\n",
       "      <td>thewdb com cool hear old walt interview http b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599690</th>\n",
       "      <td>1</td>\n",
       "      <td>2193601991</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>bpbabe</td>\n",
       "      <td>Are you ready for your MoJo Makeover? Ask me f...</td>\n",
       "      <td>0.3612</td>\n",
       "      <td>readi mojo makeov ask detail</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599691</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602064</td>\n",
       "      <td>Tue Jun 16 08:40:49 PDT 2009</td>\n",
       "      <td>tinydiamondz</td>\n",
       "      <td>Happy 38th Birthday to my boo of alll time!!! ...</td>\n",
       "      <td>0.6784</td>\n",
       "      <td>happi th birthday boo alll time tupac amaru sh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1599692</th>\n",
       "      <td>1</td>\n",
       "      <td>2193602129</td>\n",
       "      <td>Tue Jun 16 08:40:50 PDT 2009</td>\n",
       "      <td>RyanTrevMorris</td>\n",
       "      <td>happy #charitytuesday @theNSPCC @SparksCharity...</td>\n",
       "      <td>0.5719</td>\n",
       "      <td>happi charitytuesday thenspcc sparkschar speak...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1599693 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         target         ids                          date             user  \\\n",
       "0             0  1467810369  Mon Apr 06 22:19:45 PDT 2009  _TheSpecialOne_   \n",
       "1            -1  1467810672  Mon Apr 06 22:19:49 PDT 2009    scotthamilton   \n",
       "2            -1  1467810917  Mon Apr 06 22:19:53 PDT 2009         mattycus   \n",
       "3            -1  1467811184  Mon Apr 06 22:19:57 PDT 2009          ElleCTF   \n",
       "4            -1  1467811193  Mon Apr 06 22:19:57 PDT 2009           Karoli   \n",
       "...         ...         ...                           ...              ...   \n",
       "1599688       1  2193601966  Tue Jun 16 08:40:49 PDT 2009  AmandaMarie1028   \n",
       "1599689       1  2193601969  Tue Jun 16 08:40:49 PDT 2009      TheWDBoards   \n",
       "1599690       1  2193601991  Tue Jun 16 08:40:49 PDT 2009           bpbabe   \n",
       "1599691       1  2193602064  Tue Jun 16 08:40:49 PDT 2009     tinydiamondz   \n",
       "1599692       1  2193602129  Tue Jun 16 08:40:50 PDT 2009   RyanTrevMorris   \n",
       "\n",
       "                                                      text  sentiment_score  \\\n",
       "0        @switchfoot http://twitpic.com/2y1zl - Awww, t...          -0.0173   \n",
       "1        is upset that he can't update his Facebook by ...          -0.7500   \n",
       "2        @Kenichan I dived many times for the ball. Man...           0.4939   \n",
       "3          my whole body feels itchy and like its on fire           -0.2500   \n",
       "4        @nationwideclass no, it's not behaving at all....          -0.6597   \n",
       "...                                                    ...              ...   \n",
       "1599688  Just woke up. Having no school is the best fee...           0.5423   \n",
       "1599689  TheWDB.com - Very cool to hear old Walt interv...           0.4376   \n",
       "1599690  Are you ready for your MoJo Makeover? Ask me f...           0.3612   \n",
       "1599691  Happy 38th Birthday to my boo of alll time!!! ...           0.6784   \n",
       "1599692  happy #charitytuesday @theNSPCC @SparksCharity...           0.5719   \n",
       "\n",
       "                                           stemmed_content  \n",
       "0        switchfoot http twitpic com zl awww bummer sho...  \n",
       "1        upset updat facebook text might cri result sch...  \n",
       "2        kenichan dive mani time ball manag save rest g...  \n",
       "3                          whole bodi feel itchi like fire  \n",
       "4                            nationwideclass behav mad see  \n",
       "...                                                    ...  \n",
       "1599688                         woke school best feel ever  \n",
       "1599689  thewdb com cool hear old walt interview http b...  \n",
       "1599690                       readi mojo makeov ask detail  \n",
       "1599691  happi th birthday boo alll time tupac amaru sh...  \n",
       "1599692  happi charitytuesday thenspcc sparkschar speak...  \n",
       "\n",
       "[1599693 rows x 7 columns]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "klib.data_cleaning(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "fb9223c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer=TfidfVectorizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "8e06e7cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\yaswa\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\sklearn\\linear_model\\_logistic.py:469: ConvergenceWarning: lbfgs failed to converge (status=1):\n",
      "STOP: TOTAL NO. of ITERATIONS REACHED LIMIT.\n",
      "\n",
      "Increase the number of iterations (max_iter) or scale the data as shown in:\n",
      "    https://scikit-learn.org/stable/modules/preprocessing.html\n",
      "Please also refer to the documentation for alternative solver options:\n",
      "    https://scikit-learn.org/stable/modules/linear_model.html#logistic-regression\n",
      "  n_iter_i = _check_optimize_result(\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-1 {\n",
       "  /* Definition of color scheme common for light and dark mode */\n",
       "  --sklearn-color-text: black;\n",
       "  --sklearn-color-line: gray;\n",
       "  /* Definition of color scheme for unfitted estimators */\n",
       "  --sklearn-color-unfitted-level-0: #fff5e6;\n",
       "  --sklearn-color-unfitted-level-1: #f6e4d2;\n",
       "  --sklearn-color-unfitted-level-2: #ffe0b3;\n",
       "  --sklearn-color-unfitted-level-3: chocolate;\n",
       "  /* Definition of color scheme for fitted estimators */\n",
       "  --sklearn-color-fitted-level-0: #f0f8ff;\n",
       "  --sklearn-color-fitted-level-1: #d4ebff;\n",
       "  --sklearn-color-fitted-level-2: #b3dbfd;\n",
       "  --sklearn-color-fitted-level-3: cornflowerblue;\n",
       "\n",
       "  /* Specific color for light theme */\n",
       "  --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, white)));\n",
       "  --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, black)));\n",
       "  --sklearn-color-icon: #696969;\n",
       "\n",
       "  @media (prefers-color-scheme: dark) {\n",
       "    /* Redefinition of color scheme for dark theme */\n",
       "    --sklearn-color-text-on-default-background: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-background: var(--sg-background-color, var(--theme-background, var(--jp-layout-color0, #111)));\n",
       "    --sklearn-color-border-box: var(--sg-text-color, var(--theme-code-foreground, var(--jp-content-font-color1, white)));\n",
       "    --sklearn-color-icon: #878787;\n",
       "  }\n",
       "}\n",
       "\n",
       "#sk-container-id-1 {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 pre {\n",
       "  padding: 0;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-hidden--visually {\n",
       "  border: 0;\n",
       "  clip: rect(1px 1px 1px 1px);\n",
       "  clip: rect(1px, 1px, 1px, 1px);\n",
       "  height: 1px;\n",
       "  margin: -1px;\n",
       "  overflow: hidden;\n",
       "  padding: 0;\n",
       "  position: absolute;\n",
       "  width: 1px;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-dashed-wrapped {\n",
       "  border: 1px dashed var(--sklearn-color-line);\n",
       "  margin: 0 0.4em 0.5em 0.4em;\n",
       "  box-sizing: border-box;\n",
       "  padding-bottom: 0.4em;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-container {\n",
       "  /* jupyter's `normalize.less` sets `[hidden] { display: none; }`\n",
       "     but bootstrap.min.css set `[hidden] { display: none !important; }`\n",
       "     so we also need the `!important` here to be able to override the\n",
       "     default hidden behavior on the sphinx rendered scikit-learn.org.\n",
       "     See: https://github.com/scikit-learn/scikit-learn/issues/21755 */\n",
       "  display: inline-block !important;\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-text-repr-fallback {\n",
       "  display: none;\n",
       "}\n",
       "\n",
       "div.sk-parallel-item,\n",
       "div.sk-serial,\n",
       "div.sk-item {\n",
       "  /* draw centered vertical line to link estimators */\n",
       "  background-image: linear-gradient(var(--sklearn-color-text-on-default-background), var(--sklearn-color-text-on-default-background));\n",
       "  background-size: 2px 100%;\n",
       "  background-repeat: no-repeat;\n",
       "  background-position: center center;\n",
       "}\n",
       "\n",
       "/* Parallel-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item::after {\n",
       "  content: \"\";\n",
       "  width: 100%;\n",
       "  border-bottom: 2px solid var(--sklearn-color-text-on-default-background);\n",
       "  flex-grow: 1;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel {\n",
       "  display: flex;\n",
       "  align-items: stretch;\n",
       "  justify-content: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  position: relative;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:first-child::after {\n",
       "  align-self: flex-end;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:last-child::after {\n",
       "  align-self: flex-start;\n",
       "  width: 50%;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-parallel-item:only-child::after {\n",
       "  width: 0;\n",
       "}\n",
       "\n",
       "/* Serial-specific style estimator block */\n",
       "\n",
       "#sk-container-id-1 div.sk-serial {\n",
       "  display: flex;\n",
       "  flex-direction: column;\n",
       "  align-items: center;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  padding-right: 1em;\n",
       "  padding-left: 1em;\n",
       "}\n",
       "\n",
       "\n",
       "/* Toggleable style: style used for estimator/Pipeline/ColumnTransformer box that is\n",
       "clickable and can be expanded/collapsed.\n",
       "- Pipeline and ColumnTransformer use this feature and define the default style\n",
       "- Estimators will overwrite some part of the style using the `sk-estimator` class\n",
       "*/\n",
       "\n",
       "/* Pipeline and ColumnTransformer style (default) */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable {\n",
       "  /* Default theme specific background. It is overwritten whether we have a\n",
       "  specific estimator or a Pipeline/ColumnTransformer */\n",
       "  background-color: var(--sklearn-color-background);\n",
       "}\n",
       "\n",
       "/* Toggleable label */\n",
       "#sk-container-id-1 label.sk-toggleable__label {\n",
       "  cursor: pointer;\n",
       "  display: block;\n",
       "  width: 100%;\n",
       "  margin-bottom: 0;\n",
       "  padding: 0.5em;\n",
       "  box-sizing: border-box;\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:before {\n",
       "  /* Arrow on the left of the label */\n",
       "  content: \"â–¸\";\n",
       "  float: left;\n",
       "  margin-right: 0.25em;\n",
       "  color: var(--sklearn-color-icon);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 label.sk-toggleable__label-arrow:hover:before {\n",
       "  color: var(--sklearn-color-text);\n",
       "}\n",
       "\n",
       "/* Toggleable content - dropdown */\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content {\n",
       "  max-height: 0;\n",
       "  max-width: 0;\n",
       "  overflow: hidden;\n",
       "  text-align: left;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content pre {\n",
       "  margin: 0.2em;\n",
       "  border-radius: 0.25em;\n",
       "  color: var(--sklearn-color-text);\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-toggleable__content.fitted pre {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~div.sk-toggleable__content {\n",
       "  /* Expand drop-down */\n",
       "  max-height: 200px;\n",
       "  max-width: 100%;\n",
       "  overflow: auto;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {\n",
       "  content: \"â–¾\";\n",
       "}\n",
       "\n",
       "/* Pipeline/ColumnTransformer-specific style */\n",
       "\n",
       "#sk-container-id-1 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator-specific style */\n",
       "\n",
       "/* Colorize estimator box */\n",
       "#sk-container-id-1 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted input.sk-toggleable__control:checked~label.sk-toggleable__label {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label label.sk-toggleable__label,\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  /* The background is the default theme color */\n",
       "  color: var(--sklearn-color-text-on-default-background);\n",
       "}\n",
       "\n",
       "/* On hover, darken the color of the background */\n",
       "#sk-container-id-1 div.sk-label:hover label.sk-toggleable__label {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "/* Label box, darken color on hover, fitted */\n",
       "#sk-container-id-1 div.sk-label.fitted:hover label.sk-toggleable__label.fitted {\n",
       "  color: var(--sklearn-color-text);\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Estimator label */\n",
       "\n",
       "#sk-container-id-1 div.sk-label label {\n",
       "  font-family: monospace;\n",
       "  font-weight: bold;\n",
       "  display: inline-block;\n",
       "  line-height: 1.2em;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-label-container {\n",
       "  text-align: center;\n",
       "}\n",
       "\n",
       "/* Estimator-specific */\n",
       "#sk-container-id-1 div.sk-estimator {\n",
       "  font-family: monospace;\n",
       "  border: 1px dotted var(--sklearn-color-border-box);\n",
       "  border-radius: 0.25em;\n",
       "  box-sizing: border-box;\n",
       "  margin-bottom: 0.5em;\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-0);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-0);\n",
       "}\n",
       "\n",
       "/* on hover */\n",
       "#sk-container-id-1 div.sk-estimator:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-2);\n",
       "}\n",
       "\n",
       "#sk-container-id-1 div.sk-estimator.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-2);\n",
       "}\n",
       "\n",
       "/* Specification for estimator info (e.g. \"i\" and \"?\") */\n",
       "\n",
       "/* Common style for \"i\" and \"?\" */\n",
       "\n",
       ".sk-estimator-doc-link,\n",
       "a:link.sk-estimator-doc-link,\n",
       "a:visited.sk-estimator-doc-link {\n",
       "  float: right;\n",
       "  font-size: smaller;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1em;\n",
       "  height: 1em;\n",
       "  width: 1em;\n",
       "  text-decoration: none !important;\n",
       "  margin-left: 1ex;\n",
       "  /* unfitted */\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted,\n",
       "a:link.sk-estimator-doc-link.fitted,\n",
       "a:visited.sk-estimator-doc-link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "div.sk-estimator:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link:hover,\n",
       ".sk-estimator-doc-link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "div.sk-estimator.fitted:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover,\n",
       "div.sk-label-container:hover .sk-estimator-doc-link.fitted:hover,\n",
       ".sk-estimator-doc-link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "/* Span, style for the box shown on hovering the info icon */\n",
       ".sk-estimator-doc-link span {\n",
       "  display: none;\n",
       "  z-index: 9999;\n",
       "  position: relative;\n",
       "  font-weight: normal;\n",
       "  right: .2ex;\n",
       "  padding: .5ex;\n",
       "  margin: .5ex;\n",
       "  width: min-content;\n",
       "  min-width: 20ex;\n",
       "  max-width: 50ex;\n",
       "  color: var(--sklearn-color-text);\n",
       "  box-shadow: 2pt 2pt 4pt #999;\n",
       "  /* unfitted */\n",
       "  background: var(--sklearn-color-unfitted-level-0);\n",
       "  border: .5pt solid var(--sklearn-color-unfitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link.fitted span {\n",
       "  /* fitted */\n",
       "  background: var(--sklearn-color-fitted-level-0);\n",
       "  border: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "\n",
       ".sk-estimator-doc-link:hover span {\n",
       "  display: block;\n",
       "}\n",
       "\n",
       "/* \"?\"-specific style due to the `<a>` HTML tag */\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link {\n",
       "  float: right;\n",
       "  font-size: 1rem;\n",
       "  line-height: 1em;\n",
       "  font-family: monospace;\n",
       "  background-color: var(--sklearn-color-background);\n",
       "  border-radius: 1rem;\n",
       "  height: 1rem;\n",
       "  width: 1rem;\n",
       "  text-decoration: none;\n",
       "  /* unfitted */\n",
       "  color: var(--sklearn-color-unfitted-level-1);\n",
       "  border: var(--sklearn-color-unfitted-level-1) 1pt solid;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted {\n",
       "  /* fitted */\n",
       "  border: var(--sklearn-color-fitted-level-1) 1pt solid;\n",
       "  color: var(--sklearn-color-fitted-level-1);\n",
       "}\n",
       "\n",
       "/* On hover */\n",
       "#sk-container-id-1 a.estimator_doc_link:hover {\n",
       "  /* unfitted */\n",
       "  background-color: var(--sklearn-color-unfitted-level-3);\n",
       "  color: var(--sklearn-color-background);\n",
       "  text-decoration: none;\n",
       "}\n",
       "\n",
       "#sk-container-id-1 a.estimator_doc_link.fitted:hover {\n",
       "  /* fitted */\n",
       "  background-color: var(--sklearn-color-fitted-level-3);\n",
       "}\n",
       "</style><div id=\"sk-container-id-1\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>LogisticRegression()</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item\"><div class=\"sk-estimator fitted sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-1\" type=\"checkbox\" checked><label for=\"sk-estimator-id-1\" class=\"sk-toggleable__label fitted sk-toggleable__label-arrow fitted\">&nbsp;&nbsp;LogisticRegression<a class=\"sk-estimator-doc-link fitted\" rel=\"noreferrer\" target=\"_blank\" href=\"https://scikit-learn.org/1.4/modules/generated/sklearn.linear_model.LogisticRegression.html\">?<span>Documentation for LogisticRegression</span></a><span class=\"sk-estimator-doc-link fitted\">i<span>Fitted</span></span></label><div class=\"sk-toggleable__content fitted\"><pre>LogisticRegression()</pre></div> </div></div></div></div>"
      ],
      "text/plain": [
       "LogisticRegression()"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log=LogisticRegression()\n",
    "log.fit(X_train_vectorized,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "1da3a2e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_x_train=log.predict(X_train_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "79f0cd4c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.784775"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.score(X_test_vectorized,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4cfd8681",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.784775"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log.score(X_test_vectorized,y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f717a343-da4c-4fce-9b4a-aca8830aeae5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2425ba29-2120-4910-ba6c-1c110221a4af",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Model Training and Evaluation\n",
    "We are focusing on creating a highly accurate sentiment analysis model. The **RandomForestClassifier** is our model of choice due to its robustness and ability to handle complex datasets.\n",
    "\n",
    "**Steps Involved**:\n",
    "1. **Data Loading**: We load the preprocessed data, ensuring that the file exists before proceeding.\n",
    "   ```python\n",
    "   df = load_processed_data('stemmed_tweets.csv')\n",
    "   ```\n",
    "\n",
    "2. **Model Training or Loading**: If a pre-trained model exists, we load it. Otherwise, we train a new model. This approach saves time and resources.\n",
    "   ```python\n",
    "   rf_model = improved_model(df)\n",
    "   ```\n",
    "   - **Feature Extraction**: TF-IDF vectorization captures the textual essence, while StandardScaler standardizes numeric features.\n",
    "   - **Model Selection**: RandomForestClassifier is used with specific parameters optimized for performance.\n",
    "\n",
    "3. **Hyperparameter Optimization**:\n",
    "   ```python\n",
    "   best_model = optimize_hyperparameters(df)\n",
    "   ```\n",
    "   - We fine-tune our model with **RandomizedSearchCV** to ensure optimal performance, exploring a range of parameters to find the best combination.\n",
    "\n",
    "4. **Full Dataset Prediction**:\n",
    "   ```python\n",
    "   full_predictions = predict_full_dataset(df, best_model)\n",
    "   ```\n",
    "   - We predict on the entire dataset in chunks, managing memory efficiently. This method ensures the model's applicability across the full data spectrum.\n",
    "\n",
    "### ðŸ’¡ Key Insights\n",
    "- **Efficient Handling**: The pipeline is designed to handle large datasets efficiently, ensuring that even massive datasets can be processed without overwhelming resources.\n",
    "- **Effective Feature Engineering**: By combining TF-IDF vectorization and scaling, we extract meaningful patterns from the data, optimizing the model's ability to make accurate predictions.\n",
    "- **Hyperparameter Tuning**: The optimization step enhances the modelâ€™s performance, ensuring it is not only accurate but also robust and generalizable.\n",
    "\n",
    "### ðŸš€ Why This Matters\n",
    "This approach not only ensures high accuracy but also offers a well-rounded and efficient solution to sentiment analysis. By leveraging powerful machine learning techniques and optimizing for performance, the model stands out in its ability to handle large-scale text data with precision. \n",
    "\n",
    "This structure provides clarity and focus, making your project not only technically sound but also easy to understand and present."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8a336f29-b9d3-4214-b709-57778be5070a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from stemmed_tweets.csv\n",
      "Improved model with RandomForest:\n",
      "Training new model...\n",
      "Model saved to sentiment_rf_model.joblib\n",
      "Runtime: 4069.38 seconds\n",
      "Accuracy: 0.9664\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "import time\n",
    "import joblib\n",
    "\n",
    "def load_processed_data(file_path):\n",
    "    if os.path.exists(file_path):\n",
    "        print(f\"Loading processed data from {file_path}\")\n",
    "        df = pd.read_csv(file_path)\n",
    "        return df\n",
    "    else:\n",
    "        raise FileNotFoundError(f\"Processed data file not found: {file_path}\")\n",
    "\n",
    "def improved_model(df, sample_size=400000, n_estimators=200, max_features=20000, model_path='sentiment_rf_model.joblib'):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading existing model from {model_path}\")\n",
    "        pipeline = joblib.load(model_path)\n",
    "    else:\n",
    "        print(\"Training new model...\")\n",
    "        \n",
    "        df_sample = df.sample(n=sample_size, random_state=42)\n",
    "        \n",
    "        X = df_sample[['stemmed_content', 'sentiment_score']]\n",
    "        y = df_sample['target']\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        preprocessor = ColumnTransformer(\n",
    "            transformers=[\n",
    "                ('text', TfidfVectorizer(max_features=max_features, ngram_range=(1, 2)), 'stemmed_content'),\n",
    "                ('num', StandardScaler(), ['sentiment_score'])\n",
    "            ])\n",
    "        \n",
    "        pipeline = Pipeline([\n",
    "            ('preprocessor', preprocessor),\n",
    "            ('classifier', RandomForestClassifier(n_estimators=n_estimators, n_jobs=-1, random_state=42))\n",
    "        ])\n",
    "       \n",
    "        pipeline.fit(X_train, y_train)\n",
    "       \n",
    "        joblib.dump(pipeline, model_path)\n",
    "        print(f\"Model saved to {model_path}\")\n",
    "    \n",
    "    X_test = df[['stemmed_content', 'sentiment_score']].sample(n=100000, random_state=42)  # Use a subset for quick evaluation\n",
    "    y_test = df['target'].loc[X_test.index]\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Runtime: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "processed_file = 'stemmed_tweets.csv'\n",
    "df = load_processed_data(processed_file)\n",
    "\n",
    "df['stemmed_content'] = df['stemmed_content'].fillna('')\n",
    "\n",
    "print(\"Improved model with RandomForest:\")\n",
    "rf_model = improved_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "8501d111-2ef4-4d71-a83f-6d2d1a27c971",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading processed data from stemmed_tweets.csv\n",
      "Loading and evaluating the saved RandomForest model:\n",
      "Loading existing model from sentiment_rf_model.joblib\n",
      "Runtime: 56.27 seconds\n",
      "Accuracy: 0.9664\n"
     ]
    }
   ],
   "source": [
    "import joblib\n",
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "\n",
    "def load_processed_data(file_path):\n",
    "    print(f\"Loading processed data from {file_path}\")\n",
    "    return pd.read_csv(file_path)\n",
    "\n",
    "def load_and_evaluate_model(df, model_path='sentiment_rf_model.joblib', sample_size=100000):\n",
    "    start_time = time.time()\n",
    "    \n",
    "    print(f\"Loading existing model from {model_path}\")\n",
    "    pipeline = joblib.load(model_path)\n",
    "    \n",
    "    # Sample a subset of data for quick evaluation\n",
    "    X_test = df[['stemmed_content', 'sentiment_score']].sample(n=sample_size, random_state=42)\n",
    "    y_test = df['target'].loc[X_test.index]\n",
    "    \n",
    "    y_pred = pipeline.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Runtime: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    return pipeline\n",
    "\n",
    "\n",
    "processed_file = 'stemmed_tweets.csv'\n",
    "df = load_processed_data(processed_file)\n",
    "\n",
    "df['stemmed_content'] = df['stemmed_content'].fillna('')\n",
    "\n",
    "print(\"Loading and evaluating the saved RandomForest model:\")\n",
    "rf_model = load_and_evaluate_model(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "79e09767-300b-4628-8b69-1788d9a7c324",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing hyperparameters or loading best model:\n",
      "No saved model found. Running hyperparameter optimization...\n",
      "Hyperparameter optimization runtime: 8304.08 seconds\n",
      "Best parameters: {'preprocessor__text__ngram_range': (1, 3), 'preprocessor__text__max_features': 10000, 'classifier__n_estimators': 200, 'classifier__min_samples_split': 5, 'classifier__min_samples_leaf': 1, 'classifier__max_depth': None}\n",
      "Best cross-validation score: 0.8356\n",
      "Test set accuracy: 0.8398\n",
      "Best model saved to best_rf_model.joblib\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "\n",
    "def optimize_hyperparameters(df, sample_size=200000, model_path='best_rf_model.joblib'):\n",
    "    if os.path.exists(model_path):\n",
    "        print(f\"Loading best model from {model_path}\")\n",
    "        best_model = joblib.load(model_path)\n",
    "        \n",
    "        X = df.sample(n=sample_size, random_state=42)[['stemmed_content', 'sentiment_score']]\n",
    "        y = df.sample(n=sample_size, random_state=42)['target']\n",
    "        _, X_test, _, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        y_pred = best_model.predict(X_test)\n",
    "        accuracy = accuracy_score(y_test, y_pred)\n",
    "        print(f\"Loaded model test set accuracy: {accuracy:.4f}\")\n",
    "        \n",
    "        return best_model\n",
    "    \n",
    "    print(\"No saved model found. Running hyperparameter optimization...\")\n",
    "    X = df.sample(n=sample_size, random_state=42)[['stemmed_content', 'sentiment_score']]\n",
    "    y = df.sample(n=sample_size, random_state=42)['target']\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    preprocessor = ColumnTransformer(\n",
    "        transformers=[\n",
    "            ('text', TfidfVectorizer(), 'stemmed_content'),\n",
    "            ('num', StandardScaler(), ['sentiment_score'])\n",
    "        ])\n",
    "    \n",
    "    pipeline = Pipeline([\n",
    "        ('preprocessor', preprocessor),\n",
    "        ('classifier', RandomForestClassifier(random_state=42))\n",
    "    ])\n",
    "    \n",
    "    param_dist = {\n",
    "        'preprocessor__text__max_features': [10000, 20000, 30000],\n",
    "        'preprocessor__text__ngram_range': [(1, 1), (1, 2), (1, 3)],\n",
    "        'classifier__n_estimators': [100, 200, 300],\n",
    "        'classifier__max_depth': [10, 20, 30, None],\n",
    "        'classifier__min_samples_split': [2, 5, 10],\n",
    "        'classifier__min_samples_leaf': [1, 2, 4]\n",
    "    }\n",
    "    \n",
    "    random_search = RandomizedSearchCV(pipeline, param_distributions=param_dist, n_iter=20, cv=3, n_jobs=-1, random_state=42)\n",
    "    \n",
    "    start_time = time.time()\n",
    "    random_search.fit(X_train, y_train)\n",
    "    end_time = time.time()\n",
    "    \n",
    "    print(f\"Hyperparameter optimization runtime: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Best parameters: {random_search.best_params_}\")\n",
    "    print(f\"Best cross-validation score: {random_search.best_score_:.4f}\")\n",
    "    \n",
    "    y_pred = random_search.predict(X_test)\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    print(f\"Test set accuracy: {accuracy:.4f}\")\n",
    "    \n",
    "    joblib.dump(random_search.best_estimator_, model_path)\n",
    "    print(f\"Best model saved to {model_path}\")\n",
    "    \n",
    "    return random_search.best_estimator_\n",
    "\n",
    "print(\"Optimizing hyperparameters or loading best model:\")\n",
    "best_model = optimize_hyperparameters(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "663f6e26-f1a4-45fd-bceb-b19777e2abf6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No saved predictions found. Running prediction on full dataset...\n",
      "Processed 100000 out of 1600000 rows\n",
      "Processed 200000 out of 1600000 rows\n",
      "Processed 300000 out of 1600000 rows\n",
      "Processed 400000 out of 1600000 rows\n",
      "Processed 500000 out of 1600000 rows\n",
      "Processed 600000 out of 1600000 rows\n",
      "Processed 700000 out of 1600000 rows\n",
      "Processed 800000 out of 1600000 rows\n",
      "Processed 900000 out of 1600000 rows\n",
      "Processed 1000000 out of 1600000 rows\n",
      "Processed 1100000 out of 1600000 rows\n",
      "Processed 1200000 out of 1600000 rows\n",
      "Processed 1300000 out of 1600000 rows\n",
      "Processed 1400000 out of 1600000 rows\n",
      "Processed 1500000 out of 1600000 rows\n",
      "Processed 1600000 out of 1600000 rows\n",
      "Full dataset prediction runtime: 655.45 seconds\n",
      "Total number of predictions: 1600000\n",
      "Accuracy on the processed part: 0.8548\n",
      "Predictions saved to full_predictions.joblib\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score\n",
    "import time\n",
    "import joblib\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def predict_full_dataset(df, best_model, predictions_path='full_predictions.joblib', chunk_size=100000):\n",
    "    if os.path.exists(predictions_path):\n",
    "        print(f\"Loading saved predictions from {predictions_path}\")\n",
    "        full_predictions = joblib.load(predictions_path)\n",
    "        \n",
    "        accuracy = accuracy_score(df['target'], full_predictions)\n",
    "        print(f\"Loaded predictions accuracy: {accuracy:.4f}\")\n",
    "        print(f\"Total number of predictions: {len(full_predictions)}\")\n",
    "        \n",
    "        return full_predictions\n",
    "    \n",
    "    print(\"No saved predictions found. Running prediction on full dataset...\")\n",
    "    start_time = time.time()\n",
    "    \n",
    "    full_predictions = []\n",
    "    for i in range(0, len(df), chunk_size):\n",
    "        chunk = df.iloc[i:i+chunk_size]\n",
    "        chunk_pred = best_model.predict(chunk[['stemmed_content', 'sentiment_score']])\n",
    "        full_predictions.extend(chunk_pred)\n",
    "        \n",
    "        if (i + chunk_size) % (chunk_size * 10) == 0:\n",
    "            joblib.dump(full_predictions, f'intermediate_predictions_{i+chunk_size}.joblib')\n",
    "        \n",
    "        print(f\"Processed {i+len(chunk_pred)} out of {len(df)} rows\")\n",
    "        \n",
    "        if time.time() - start_time > 3500:  # 3500 seconds = 58 minutes\n",
    "            print(\"Approaching 1 hour limit. Saving current progress.\")\n",
    "            break\n",
    "    \n",
    "    full_predictions = np.array(full_predictions)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    print(f\"Full dataset prediction runtime: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Total number of predictions: {len(full_predictions)}\")\n",
    "    \n",
    "    accuracy = accuracy_score(df['target'][:len(full_predictions)], full_predictions)\n",
    "    print(f\"Accuracy on the processed part: {accuracy:.4f}\")\n",
    "    \n",
    "    joblib.dump(full_predictions, predictions_path)\n",
    "    print(f\"Predictions saved to {predictions_path}\")\n",
    "    \n",
    "    return full_predictions\n",
    "\n",
    "full_predictions = predict_full_dataset(df, best_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42103691-dbb3-4ed0-af8b-18d0f80961d6",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Project Overview: Twitter Sentiment Analysis\n",
    "In this project, we focused on analyzing the sentiment of Twitter data using advanced natural language processing (NLP) techniques. The goal was to build a robust pipeline that could handle large datasets, perform accurate sentiment analysis, and provide insightful predictions.\n",
    "\n",
    "### ðŸ’¡ Key Contributions and Insights\n",
    "- **Data Preprocessing Excellence**: \n",
    "  - **Stemming**: Leveraged the Porter Stemmer to reduce words to their root forms, effectively minimizing the vocabulary size while retaining the essence of the content.\n",
    "  - **Stopwords Removal**: Eliminated non-essential words using NLTK's stopwords list, ensuring the focus remained on the most impactful terms.\n",
    "  - **Handling Null Values**: Carefully inspected and managed null values in the `stemmed_content` field, ensuring data integrity throughout the process.\n",
    "  \n",
    "- **Efficient Data Management**: \n",
    "  - Successfully processed and vectorized a large dataset, optimizing memory usage and ensuring the pipeline could handle extensive Twitter data without compromising on performance.\n",
    "\n",
    "- **Advanced Feature Extraction**:\n",
    "  - **TF-IDF Vectorization**: Extracted critical features from the text, focusing on the top 5,000 terms that contribute most significantly to the sentiment classification. This step enhanced the model's ability to distinguish between different sentiments.\n",
    "\n",
    "### ðŸš€ Performance Highlights\n",
    "- **Model Training and Validation**: \n",
    "  - The data was split into training, validation, and test sets to ensure the model's performance was not only accurate but also generalizable.\n",
    "  - **Vectorization Success**: The vectorized training data showed a shape of `(X_train_vectorized.shape)`, demonstrating the efficiency of our feature extraction process.\n",
    "  \n",
    "- **Impactful Outcomes**:\n",
    "  - **Memory Efficiency**: Achieved a memory usage of just `memory_usage` MB, highlighting the pipeline's capability to handle large-scale data efficiently.\n",
    "  - **Accurate Predictions**: The process concluded with a model that is well-prepared to deliver accurate and insightful sentiment predictions on new Twitter data.\n",
    "\n",
    "### ðŸ† Why This Matters\n",
    "This project showcases a meticulous approach to NLP and sentiment analysis, emphasizing efficiency, accuracy, and scalability. By crafting a solution that handles large datasets with ease and delivers reliable sentiment predictions, this work stands out as a prime example of effective data-driven problem-solving.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
